---
title: "Linear Model -- Lab"
author: "Menghan Yuan"
date: "Sept 11, 2024"
output: 
    bookdown::html_document2:
        mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML.js"
        self_contained: false
        toc: true
        toc_float: true
        toc_depth: 4
        number_sections: false
        css: "style.css"
editor_options: 
  chunk_output_type: console
---

<SCRIPT language="JavaScript" SRC="my_jxscript.js"></SCRIPT>

<a class="top-link" href="#top" id="js-top">â†‘</a>

```{r setup, include=F, echo=F}
library(knitr) # load packages
library(kableExtra)
library(tidyverse)
library(latex2exp)
library(stargazer)
library(bookdown)
library(quantmod)

# don't show code unless we explicitly set echo = TRUE
opts_chunk$set(echo = TRUE, message=FALSE, fig.align="center", fig.pos = "H")
opts <- options(knitr.kable.NA = "")

## control long outputs by using eg `max.lines = 10`
hook_output_default <- knitr::knit_hooks$get('output')
truncate_to_lines <- function(x, n) {
   if (!is.null(n)) {
      x = unlist(stringr::str_split(x, '\n'))
      if (length(x) > n) {
         # truncate the output
         x = c(head(x, n), '...\n')
      }
      x = paste(x, collapse = '\n') # paste first n lines together
   }
   x
}
knitr::knit_hooks$set(output = function(x, options) {
   max.lines <- options$max.lines
   x <- truncate_to_lines(x, max.lines)
   hook_output_default(x, options)
})
## evaluate in the current global environment
# rmarkdown::render("/Users/menghan/Library/CloudStorage/OneDrive-Norduniversitet/FIN5005 2024Fall/CoreMetrics/05_Sept_11_OLS-lab.Rmd", envir=.GlobalEnv)
```


# Regression Dataset Preparation 

Using `AAPL` as an example to demonstrate linear regression.

We use monthly data from 2014-12 to 2023-12. 

- Equity data can be downloaded using `quantmod::getSymbols`.
- Fama-French factors can be downloaded from Ken French's [website][FF-factor].


```{r}
price_data <- getSymbols("AAPL", 
           src = 'yahoo', 
           from = "2014-12-01", 
           to = "2023-12-31",   
           auto.assign = FALSE
           )
price_data
```

Convert to monthly and calculate returns.

```{r}
price_data <- price_data %>% 
    apply.monthly(last)
price_data$return <- monthlyReturn(price_data$AAPL.Adjusted, type='arithmetic')
price_data
```

Load Fama-French factors.

```{r}
FF_factor <- read_csv("data/FF_3Factors_US_monthly.csv")
FF_factor <- FF_factor %>% mutate_at(vars(-Date), ~./100)
FF_factor <- FF_factor %>% 
    mutate(year=year(Date),
           mon=month(Date))

FF_factor
```

Merge price data with FF-factor data to prepare the regression dataset.

```{r}
reg_data <- price_data %>% 
    as_tibble() %>% 
    add_column(Date=index(price_data), .before = 1)
reg_data <- reg_data %>% 
    mutate(year=year(Date),
           mon=month(Date))
reg_data <- reg_data %>% 
    left_join(FF_factor[,-1], by=c("year","mon"))
# calculate excess return
reg_data <- reg_data %>% 
    mutate(eRi = return-RF) %>% 
    rename(rmrf=`Mkt-RF`)
```


```{r}
# glimpse of data
reg_data %>% 
    select(-year,-mon) %>% 
    knitr::kable(floating.environment="sidewaystable", digits = 5, escape=F) %>%
    kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, latex_options="scale_down") %>% 
    scroll_box(width = "100%", height = "500px")
```


Visualize excess returns of Apple and equity risk premium $(R_m-R_f)$.

- Give you a nice overview of the data.
- Spot any irregular patterns and do due diligence to ensure data quality. For instance, to check whether you got the unit right, missing values treatment.

```{r fig.keep=2}
plot_data <- xts(reg_data[,c("eRi","rmrf")], order.by = reg_data$Date)
plot_data
col_vec <- c("black", "red")
plot.xts(plot_data, col = col_vec, main = "Excess Returns on Asset and Market")
addLegend("topright", 
          legend.names = c("AAPL", "Equity Risk Premium"), 
          lty = c(1, 1), 
          lwd = c(2, 2),
          col = col_vec,
          bg = "white",
          box.col = "white")

```



___

# Simple linear regression -- CAPM

Simple linear regression using CAPM use an example.

CAPM as a regression can be expressed as

$$
r_{i,\color{red}{t}} - r_{f,\color{red}{t}} = \alpha_i + \beta_i (r_{m,\color{red}{t}}-r_{f,\color{red}{t}}) + \varepsilon_{i,\color{red}{t}}
$$ 
where $\alpha_i$ (intercept) and $\beta_i$ (slope) are the parameters to estimate.

We use the following data to estimate the parameters.

-   $r_i$ is the return of asset $i$;
-   $r_f$ is the risk free rate of interest;
-   $r_m$ is the return of the market portfolio.

```{r CAPM}
capm_ml <- lm(eRi~rmrf, data=reg_data)
summary(capm_ml)
```

`stargazer` tabulates regression results neatly.


```{r, echo=FALSE, results='asis'}
stargazer(capm_ml, digits=4, type="html", 
          title="Regression Results for AAPL", 
          notes="<span>&#42;&#42;&#42;</span>: p<0.01; <span>&#42;&#42;</span>: <strong>p<0.05</strong>; <span>&#42;</span>: p<0.1 <br> Standard errors in the parentheses.", 
          notes.append=F)
```


Note that is is common practice to put standard error in the parentheses below the coefficient estimates. 

- Sometimes, $t$-statistics are reported in parentheses. 
- To avoid any misunderstanding, it is recommended to specify in the table footnote what is enclosed in parentheses.


Several things worth looking at:

-   Explanatory power: the $R^2$ ($0.48$) tells us that variation in the excess returns of the market index explains about $48\%$ of the variation in Apple's returns. The adjusted $R^2$ (which is slightly smaller) imposes a penalty for extra number of predictors.

-   The intercept ($0.0103$) represents the estimate of Apple's alpha, with the numbers in parentheses ($0.0058$) being the standard error (SE).

    - We test $H_0: \alpha=0$, against $H_1: \alpha \ne 0$. We consider a two-sided t-test at the $5\%$ significance level.
    
    - <span style='color:#337ab7'>Test statistics</span>: $t = \frac{0.0103}{0.0058} = 1.78 \sim t_{107}$, following a Student's *t*-distribution with $107$ degrees of freedom ($df$). We use the subsript to denote the $df$. 
    
    - <span style='color:#337ab7'>The critical value</span> is the $(1-\frac{\tau}{2})$ quantile of the *t*-distribution, where $\tau=5\%$ is the significance level. It can be calculated as the following.
        
        ```{r}
        tau <- 0.05 # significance level 
        qt(p=1-tau/2, df=107)
        ```
        
        Equivalently, by symmetry of the *t*-distribution, $t_{1-\tau/2, 107}=-t_{\tau/2, 107}$, we have
            
        ```{r}
        -qt(p=tau/2, df=107)
        ```
    
        Notation clarification: $t$ refers to both the name of the distribution and the statistic. Have to determine by the context. Sometimes, you see two subscripts, $t_{\alpha, v}$, where the first subscript ($\alpha$) denotes the significance level, and the second subscript ($v$) denotes the $df$.
    
    - <span style='color:#337ab7'>Rejection rule</span>. Our *t*-statistic is less then the critical value ($1.78<1.98$), therefore we fail to reject $H_0: \alpha=0$.
    
    - Alternatively, we may check the *p*-value. If the *p*-value is smaller than the significance level, we reject $H_0$. \
    The *p*-value is the probability of observing a test-statistic value at least as "extreme" as $t$ if null hypothesis $H_{0}$ were true. Formally speaking,
    $$
    \begin{aligned}
    p\text{-value} &= 2\,\mathbb{P}(T>|t|)   &\quad& (\text{by definition, the probability of the two tails})   \\
    &= 2\left(1-\mathbb{P}(T\le|t|)\right)   & & (2 \text{ times the probability of the upper tail})  \\
    &= 2\mathbb{P}(T\le -|t|)  & & (2 \text{ times the probability of the lower tail}) .
    \end{aligned} 
    $$
    which can be calculated as:  
    
        ```{r}
        # using upper tail
        2*(1-pt(q=1.78, df=107))
        # this is same as using lower tail
        2*pt(q=-1.78, df=107)
        ```
    
        We see that the *p*-value is $7.79\%$, which is larger than the $5\%$ significance level. We reject the $H_0$.
    
    - Note that the *t*-distribution approximates to the shape of a standard normal distribution when the degrees of freedom are high. In our case, we have $df=107$, which is sufficiently high. We could use the critical value of the standard normal distribution for convenience. 
    
    - Let's check now what the critical value would be if we use the normal distirbution:
        ```{r}
        qnorm(p=1-tau/2)
        ```
        Equivalently,
        ```{r}
        -qnorm(p=tau/2)
        ```
        which is close to the critical value under the *t*-distribution.
        
        What the $p$-value would be if we use the normal distribution
        ```{r}
        2*pnorm(q=-1.78)
        ```
        which is $7.51\%$, fairly close to the $p$-value under the *t*-distribution.
        
    
    -  A common rule of thumb is that for $df > 30$, one can use the normal distribution in place of a *t*-distribution unless you are interested in the extreme tails. Since $df=n-K$, where $n$ is the sample size and $K$ is the number of covariates. We could say that with a large sample size $n$, the difference between the normal and the $t$-distribution is not pronounced.
    
    - The power of *t*-distribution is that it adjusts for smaller sample sizes by having a more conservative estimate of probability density (fatter tails). This can be shown by the larger critical value under *t*-distribution (1.98) compared to the normal distribution (1.96).
    
    - In short, the use of the exact t-distributions, in place of normal approximations, is not as crucial as you might be led to believe from a reading a some text books. The big differences between the exact t-distribution and the normal approximation arise only for quite small sample sizes.

-   The slope ($1.2$) is the estimate of beta, meaning that its share price tended to move 1.2% for every 1% move in the market index.

    -   $\beta>1$ means above-average sensitivity to economy, high exposure to the state of macroeconomy, high-risk equities.

    -   The SE is $0.12$. P-value in the table provides the significance level for $H_0: \beta=0$. The p-value is basically zero, therefore, we reject $H_0$ in favor of $H_1$.

    -   An alternative t-test would be $H_0: \beta=1$. 
        - Test statistics can be calculated as: 
            $$
            t = \frac{\hat{\beta}-\beta}{se_{\hat{\beta}}} = \frac{1.2-1}{0.12} = 1.67,
            $$
            which is smaller than the critical value of $1.98$. Therefore we fail to reject $H_0$. $\beta$ is not significantly different from one.
        - Alternatively, we can obtain the same result using the p-value can be obtained by

            ```{r}
            pt(q = -1.67, df=107) * 2
            ```
            which is larger than the significant level. Therefore, we fail to reject $H_0: \hat{\beta}=1$ at 5% significance level.

    -   Sometimes we are interested in the $95\%$ *Confidence Interval* (CI) that includes the true but unobserved value of beta with 95% probability.

        We would take the estimated value as the center of the interval, and then add and subtract about two standard errors.

        More precisely, the $95\%$ CI is given as $[\hat{\beta}-t_{1-\tau/2, v} \cdot se_{\hat{\beta}}, \hat{\beta}+t_{1-\tau/2, v} \cdot se_{\hat{\beta}}]$.
        
        Plug in $t_{1-\tau/2, v}=1.98$, we have 
        ```{r}
        c("lower bound"=1.2-1.98*0.12, "upper bound"=1.2+1.98*0.12) # 95% CI
        ```
        
        Alternatively, if using the normal distribution approximation, the $95\%$ CI is given as $[\hat{\beta}-1.96 \cdot se_{\hat{\beta}}, \hat{\beta}+1.96 \cdot se_{\hat{\beta}}]$.
        
        ```{r}
        qnorm(p = 0.025) # critical value of 1.96
        c("lower bound"=1.2-1.96*0.12, "upper bound"=1.2+1.96*0.12) # 95% CI
        ```
        
        Note that $0$ is not included in the 95\% CI, yet $1$ is included.
        
    -   The standard deviation of the residual is $6\%$ per month, or $6\% \times \sqrt{12}=20.78\%$. This stands for firm-specific risk.


___

# Multivariate linear regression -- Fama-French

The Fama-French three-factor model as a regression can be expressed as

$$
r_{i,\color{red}{t}} - r_{f,\color{red}{t}} = \alpha_i + \beta_i^{RMRF} (r_{m,\color{red}{t}}-r_{f,\color{red}{t}}) + \beta_i^{SMB}SMB_\color{red}{t} + \beta_i^{HML}HML_\color{red}{t} + \varepsilon_{i,\color{red}{t}}
$$ 




```{r FF}
FF_ml <- lm(eRi~rmrf+SMB+HML, data=reg_data)
summary(FF_ml)
```



```{r, echo=FALSE, results='asis'}
stargazer(capm_ml, FF_ml, type="html", 
          title="Regression Results for AAPL", 
          align = TRUE, notes="<span>&#42;&#42;&#42;</span>: p<0.01; <span>&#42;&#42;</span>: <strong>p<0.05</strong>; <span>&#42;</span>: p<0.1 <br> Standard errors in the parentheses.", notes.append=F)
```


&nbsp;

- Under the FF 3-factor model, equity risk premium is still a significant factor with a slightly larger value (1.269 vs 1.204).
- SMB size factor is not significant.
- HML value factor is significantly negative, indicating Apple is a growth company with low HML.
- Larger explanatory power implied by Adjusted $R^2$ (54.8% vs 47.6%).


```{r}
library(broom)
coef <- tidy(FF_ml, conf.int = TRUE)
coef %>% 
    filter(term != "(Intercept)") %>% 
    ggplot(aes(x = term, y = estimate) ) +
    geom_point() +
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
    labs(title = "FF 3-Factor Coefficients for AAPL",
         y = "Coefficient") +
    theme(axis.title.x = element_blank())
```


<!--End of script-------------------------------------------------------------->
[FF-factor]: http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html










