---
title: "Binary Choice Model"
author: "Menghan Yuan"
date: "Oct 25, 2024"
output: 
    bookdown::html_document2:
        mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML.js"
        self_contained: false
        toc: true
        toc_float: true
        toc_depth: 4
        number_sections: false
        df_print: paged
        css: "style.css"
editor_options: 
  chunk_output_type: console
---

<SCRIPT language="JavaScript" SRC="my_jxscript.js"></SCRIPT>

<a class="top-link" href="#top" id="js-top">↑</a>

```{r setup, include=F, echo=F}
library(knitr) # load packages
library(kableExtra)
library(tidyverse)
library(latex2exp)
library(stargazer)
library(bookdown)
library(cowplot)
library(scales)

# don't show code unless we explicitly set echo = TRUE
opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align="center", fig.pos = "H", paged.print=FALSE)
opts <- options(knitr.kable.NA = "")

## control long outputs by using eg `max.lines = 10`
hook_output_default <- knitr::knit_hooks$get('output')
truncate_to_lines <- function(x, n) {
   if (!is.null(n)) {
      x = unlist(stringr::str_split(x, '\n'))
      if (length(x) > n) {
         # truncate the output
         x = c(head(x, n), '...\n')
      }
      x = paste(x, collapse = '\n') # paste first n lines together
   }
   x
}
knitr::knit_hooks$set(output = function(x, options) {
   max.lines <- options$max.lines
   x <- truncate_to_lines(x, max.lines)
   hook_output_default(x, options)
})
plot_png <- function(p, f_name, width, height, ppi=300){
    # a plot wrapper
    png(f_name, width=width*ppi, height=height*ppi, res=ppi)
    print (p)
    dev.off()
}
```




A limited dependent variable (LDV) $y$ takes values in some “limited” set. Examples:

- Binary: $y\in \{0,1\}$
- Integer: $y\in \{0,1,2, \ldots\}$
- Censored: $y\in \mathbb R^+$


We will focus on the binary dependent variable case for our course.

# Introductory Example

The western collaborative group study (WCGS) was a large epidemiological study designed to investigate the risk factors and coronary heart disease (CHD).
More than 3000 men were recruited to the study, and a number of (potential) risk factors were recorded at entry. The men were then followed for about ten years and it was recorded if they developed CHD or not. 

In this introductory example, we will study **the effect of smoking and age on the risk for CHD**.

```{r paged.print=TRUE}
wcgs <- read.table("https://raw.githubusercontent.com/my1396/course_dataset/refs/heads/main/wcgs.txt", sep="\t", header=TRUE, na.strings=".")
wcgs %>% head(20)
```

```{r}
wcgs %>% select(chd69, age) %>% str()
```

We first look at the effect of smoking (predictor) on CHD (outcome).

- `smoke` is a binary covariate taking values either 0 (`non-smoker`) or 1 (`smoker`). 

- `chd69` is a binary covariate indicating whether a person developed CHD, 0 for `non-developed` and 1 for `developed.`


A <span style='color:#337ab7'>**contingency table**</span> is an effective method to see the association between two categorical variables. It counts the number of observations in each of the four possible scenarios.
When dealing with just one categorical variable, this is referred to as a **frequency table**, which count the number of observations for each category.


We start out by making a 2x2 contingency table for the development of CHD or not for smokers and nonsmokers:

```{r}
contingency_data <- wcgs %>% 
    select(smoke, chd69) %>% 
    mutate(smoke = factor(smoke, levels=c(0,1), labels=c("non-smoker","smoker")),
           chd69 = factor(chd69, levels=c(0,1), labels=c("non-developed", "developed")))
contingency_table <- with(contingency_data, table(chd69, smoke))
contingency_table
```

We can display row and column totals if needed.
```{r}
contingency_table %>% addmargins()
```

- We see that there are 1502 smokers and that 159 of these developed CHD.
- Of the 1652 non-smokers, 98 developed CHD.


Visualization of the contingency table

```{r echo=FALSE, include=FALSE}
p <- contingency_table %>% 
       as_tibble() %>% 
       mutate(chd69 = factor(chd69, 
                             levels=c("non-developed", "developed"))) %>%
    ggplot(aes(x=smoke, y=n, fill=chd69)) +
    geom_bar(position="stack", stat="identity", color="black", linewidth=0.1) + 
    scale_fill_grey(start=0.88, end=0.7) +
    labs(y="Frequency", fill="CHD") +
    theme(axis.title.x = element_blank(), 
          axis.title = element_text(size=rel(1.2)),
          axis.text = element_text(size=rel(1.2)),
          legend.position = "bottom",
          legend.text = element_text(size=rel(1.2)),
          legend.title = element_text(size=rel(1.2)) 
          )
f_name <- "images/stacked_bar.png"
plot_png(p, f_name, 5.17, 5)
```


```{r echo=FALSE, out.width = "50%", fig.cap="Stacked bar plot"}
include_graphics(f_name)
```


The <span style='color:#337ab7'>stacked bar graph</span> shows

- the sample sizes of smokers and non-smokers;
- the distribution of developed vs non-developed within each category.


The <span style='color:#337ab7'>mosaic plot</span> replaces absolute frequencies (counts) with relative frequencies within each category.
It represents each combination of the variables as a rectangle, and the size of the rectangle is proportional to the number of individuals in that combination.

The advantage of mosaic plots is that it demonstrates the difference in estimated conditional probabilities among groups.


```{r}
mosaicplot(contingency_table %>% t(), 
           xlab="Smoke", ylab="CHD", 
           main="Mosaic plot: Smoke X CHD", cex.axis = 1.2)
```


- The widths of the boxes are proportional to the percentage of non-smokers and smokers, respectively. In fact, $52.38\%$ of applicants were non-smokers and $47.62\%$ were smokers.
- The heights of the boxes are proportional to percent developed. We can see that non-smokers' box for the developed on the left is shorter than the smokers’ developed box on the right.
In fact, $5.93\%$ of the non-smokers developed CHD, while $10.59\%$ of the smokers developed CHD. That is, there is a larger proportion of people that developed CHD for smokers compared to non-smokers.


We can show the proportions of developing CHD for the two groups (smokers vs. non-smokers) in Table \@ref(tab:risk-table).

```{r}
risk_table <- contingency_table %>% 
            prop.table(margin=2) %>% 
            as.data.frame.matrix() 
```


```{r risk-table, echo=FALSE}
risk_table %>% 
    as_tibble(rownames = " ") %>% 
    add_column("  " = c("CHD", NA), .before=1) %>% 
    mutate_at(vars(-1:-2), percent, accuracy=0.01) %>% 
    kable(align = "clcc", caption="Risk of developing CHD for non-smokers and smokers.") %>% 
    kable_styling(bootstrap_options = c("striped", "hover"), full_width = F) %>% 
    add_header_above(c(" "=2, "Smoke"=2)) %>% 
    column_spec(1:2, bold = TRUE) 
```


- For non-smokers, $5.9\%$ developed CHD.
- For smokers, $10.6\%$ developed CHD.

___

## Measure or Risk and Association for Binary Outcomes

- The *risk* refers to outcome risk for individuals who are smokers or non-smokers and are simply the observed proportions of individuals with CHD in these groups.

    - In our case, the risk for non-smokers (`smoke=0`, control group) is the proportion who developed CHD among non-smokers, which is $5.9\%$, denoted as $P(0)$. 
    - The risk for smokers (`smoke=1`, treatment group) is the proportion who developed CHD among smokers, which is $10.6\%$, deonoted as $P(1)$.
    
- The <span style='color:#008B45FF'>*risk difference*</span> or <span style='color:#008B45FF'>*excess risk*</span> (ER) is defined as the difference between the estimated risk in the groups defined by the predictor. 
    $$
    ER = P(1)- P(0)
    $$
    - If $ER=0$, there is no difference in risk between the two groups.
    
    - If $ER>0$, the risk is higher for the treatment group compared to the control group.
    
    - If $ER<0$, the risk is lower for the treatment group compared to the control group.
    
    Because the component risks range between zero and one, the ER can take on values between $-1$ and $1$, i.e.,
    $$
    -1 < ER < 1
    $$
    
    - For our contingency table, we can see that the risk difference is
        $$
        10.6\%-5.9\%=4.7\%
        $$
        
        Can be verified as:
        ```{r}
        p1 <- risk_table[2,2]
        p0 <- risk_table[2,1]
        ER <- p1-p0
        ER
        ```
    
- The <span style='color:#008B45FF'>*risk ratio*</span> or <span style='color:#008B45FF'>*relative risk*</span> (RR) is the ratio of these risks.

    $$
    RR = \frac{P(1)}{P(0)}
    $$
    
    - If $RR=1$, there is no difference in risk between the two groups.
        
    - If $RR>1$, the risk is higher for the treatment group compared to the control group.
    
    - If $RR<1$, the risk is lower for the treatment group compared to the control group.
    
    $RR$ is a relative measure, and can only take nonegative values.
    
    $$
    RR\ge 0
    $$

    - In our example, the risk ratio is
        $$
        \frac{10.6\%}{5.9\%}=1.8
        $$
        
        Can be verified as
        ```{r}
        RR <-  p1/p0
        RR
        ```
    
    
- The <span style='color:#008B45FF'>**odds ratio**</span> (OR) is the ratio between the corresponding odds in the two groups.
    - The <span style='color:#008B45FF'>**odds**</span> of an outcome occurring are computed as the probability of *occurrence* divided by the complementary probability that the event does *not occur*. 
    
        Given a probability $p$, the odds is given by
        
        $$
        \text{odds} = \frac{p}{1-p}
        $$
        
        Since the denominators of these two probabilities are identical, the odds can be also be calculated as the ratio of the number of outcomes to nonoutcomes. 
        
        For example, getting heads with a fair coin has *even odds*, that is, the probability of getting a head is $50\%$.
    
    - The odds of CHD occurrence in the smokers groups are 
        $$
        \begin{align}(\#eq:odds1)
        \text{odds}(smoke=1) = \frac{P(1)}{1-P(1)} = \frac{10.6\%}{1-10.6\%} = 11.84\%
        \end{align}
        $$
        
        Can be verified as
        ```{r}
        p1/(1-p1)
        ```
        
    - The odds of CHD occurrence in the non-smokers groups are 
        $$
        \begin{align}(\#eq:odds0)
        \text{odds}(smoke=0) = \frac{P(0)}{1-P(0)} = \frac{5.9\%}{1-5.9\%} = 6.31\%
        \end{align}
        $$
        
        Can be verified as
        ```{r}
        p0/(1-p0)
        ```
    
    - The odds ratio is the ratio between the corresponding odds in the two groups.
        
        $$
        OR = \frac{P(1)/[1-P(1)]}{P(0)/[1-P(0)]}
        $$
        
        - If $OR=1$, the treatment does not affect odds of outcome. 
        - If $OR>1$, the treatment groups has higher odds of outcome. 
        - If $OR<1$, the treatment groups has lower odds of outcome. 
        
        
        $OR$ is nonnegative.
        
        $$
        OR>0
        $$
    
    - In our contingency table, the odds ratio is given by
    
        $$
        \frac{11.84\%}{6.31\%} = 1.88
        $$
        
        Can be verified as
        ```{r}
        OR <- (p1/(1-p1))/(p0/(1-p0))
        OR
        ```
        
___

From a fitted logistic model we may estimate the odds ratio corresponding to a one unit's increase in a covariate. Use this result to compute the odds ratio for CHD for a smoker compared to a non-smoker.

## Logistic Regression with a Single Predictor

We then use a logistic regression model with smoke as covariate.

Mathematical representation:
$$
\begin{align} (\#eq:ex1)
p(x) = P(y=1\vert x) = \frac{\exp(\beta_0+\beta_1 x)}{1+\exp(\beta_0+\beta_1 x)}
\end{align}
$$

We need to reformulate Eq. \@ref(eq:ex1) for the interpretation so that only the linear term is on the right side of the equation. 

$$
\begin{align} (\#eq:ex1-log)
\log \left[\frac{p(x)}{1-p(x)} \right] = \beta_0+\beta_1 x \,.
\end{align}
$$

The expression on the LHS is called the log odds.

Taking the exponential to Eq. \@ref(eq:ex1-log), we obtain:
$$
\begin{align}
\frac{p(x)}{1-p(x)} = \exp (\beta_0+\beta_1 x) \,.
\end{align}
$$

Note that the LHS is actually the odds. 

In logistic regression, if we take <span style='color:#337ab7'>the exponential of the coefficient</span>, it is the <span style='color:#337ab7'>**odds ratio corresponding to one unit's increase in the value of the covariate**</span>, holding other variables constant.


```{r}
fit.smoke <- glm(chd69~smoke, data=wcgs, family=binomial)
summary(fit.smoke)
```

The regression result can be summarized in Table \@ref(tab:fit-smoke).
```{r fit-smoke, echo=FALSE, results='asis', fig.cap="Logistic regression of CHD on smoke."}
tit <- knitr::opts_current$get("fig.cap")
tit_html <- paste0('<span id="tab:',
                   knitr::opts_current$get("label"),
                   '">(#tab:',
                   knitr::opts_current$get("label"),
                   ')</span>',
                   tit)
stargazer(fit.smoke, type="html",
          digits=4,
          label = paste0("tab:", knitr::opts_current$get("label")),
          title = ifelse(knitr::is_latex_output(), tit, tit_html),
          notes="<span>&#42;</span>: p<0.1; <span>&#42;&#42;</span>: <strong>p<0.05</strong>; <span>&#42;&#42;&#42;</span>: p<0.01 <br> Standard errors in parentheses.",
          notes.append=F )
```


In this case, <span style='color:#337ab7'>the odds ratio for CHD for a smoker compared to a non-smoker</span> can be obtained by

$$
e^{0.63} = 1.88\, ,
$$
which is exactly the same number as we calculated using the contingency table.

More interpretations: the odds ratio being $1.88$ means that <span style='color:#337ab7'>the odds of a smoker developing CHD are $1.88$ times greater than the odds of a non-smoker developing CHD</span>.

More formally, if you increase the value of covariate $x_1$ by one unit, the <span style='color:#337ab7'>estimated odds change by a factor of $\exp(\beta_1)$</span>.

We know that the odds for non-smokers are $6.31\%$ from Eq. \@ref(eq:odds0). Given the odds ratio of $1.88$, this indicates that the **expected** odds for smokers to develop CHD is
$$
\begin{aligned}
\text{odds}\,(smoker=1) &=  \text{odds}\,(smoker=0) \times \text{odds ratio}  \\
&=6.3\% \times 1.88  \\
&= 11.84
\end{aligned}
$$
This can be verified as
```{r}
# odds(smoker=0) * exp(beta1)
p0/(1-p0) * exp(fit.smoke$coefficients[2]) 
```


**Interpretation of intercept**: The exponential of $\beta_0$ is the estimated odds when the categorical covariates are at the reference level and all numerical covariates are zero. 

In our case, the estimated odds for a non-smoker to develop CHD is
$$
\text{odds}\,(smoker=0) = e^{-2.764} = 6.3\%
$$



___


We now consider a numerical predictor, `age`.
We then use logistic regression to study the effect of age for the risk of developing CHD:

```{r}
fit.age <-  glm(chd69~age, data=wcgs, family=binomial)
summary(fit.age)
```

The regression result can be summarized in Table \@ref(tab:fit-age).
```{r fit-age, echo=FALSE, results='asis', fig.cap="Logistic regression of CHD on age."}
# Use title caption from fig.cap
tit <- knitr::opts_current$get("fig.cap")
# Adding caption for html output
tit_html <- paste0('<span id="tab:',
                   knitr::opts_current$get("label"),
                   '">(#tab:',
                   knitr::opts_current$get("label"),
                   ')</span>',
                   tit)
stargazer::stargazer(fit.age,
          digits=4,
          label = paste0("tab:", knitr::opts_current$get("label")),
          title = ifelse(knitr::is_latex_output(), tit, tit_html),
          type = ifelse(knitr::is_latex_output(),"latex","html"),
          notes = "<span>&#42;</span>: p<0.1; <span>&#42;&#42;</span>: <strong>p<0.05</strong>; <span>&#42;&#42;&#42;</span>: p<0.01 <br> Standard errors in parentheses.",
          notes.append = F,
          header = F
          )
```


&nbsp;

- The odds ratio for one year increase in age is $e^{0.0744} = 1.077\, .$ This means that for each additional year of age, the odds increase by a factor of $1.077.$
- The odds ratio for a ten-year increase is $e^{0.0744\times 10} = 2.10\, .$ That is, the odds are $2.10$ times higher for an additional ten years of age.

Can be verified using the following codes.
```{r}
expcoef <- function(glmobj){
    # computing odds ratio with 95% confidence limits
    regtab <- summary(glmobj)$coef 
    expcoef <- exp(regtab[,1]) 
    lower <- expcoef * exp(-1.96*regtab[,2]) 
    upper <- expcoef * exp(1.96*regtab[,2]) 
    return (cbind(expcoef,lower, upper))
} 
```

Table \@ref(tab:OR-age) shows the odds ratio for age and its $95\%$ confidence interval. 

```{r OR-age}
expcoef(fit.age)[-1,,drop=FALSE] %>% 
    kable(digits = 4, escape= TRUE, 
          caption = "Exponential of the coefficient and 95% CI.") %>% 
    kable_styling(full_width = FALSE)
```


___

## Logistic Regression with Several Predictors

Consider the WCGS study with CHD as outcome and age, cholesterol (mg/dL), systolic blood pressure (mmHg), body mass index (kg/m2), and smoking (0/1) as predictors.


Mathematical representation:
$$
\begin{align} 
p(\boldsymbol x) = P(y=1\vert \boldsymbol x) = \frac{\exp(\beta_0+\beta_1 x_2 + \cdots +\beta_K x_K)}{1+\exp(\beta_0+\beta_1 x_2+ \cdots +\beta_K x_K)}
\end{align} (\#eq:ex-multi)
$$
The odds given $\boldsymbol{x}=[1, x_2, \ldots, x_K]^T$ can be obtained by:
$$
\log \left[\frac{p(\boldsymbol x)}{1-p(\boldsymbol x)}\right] = \beta_0+\beta_1 x_x + \cdots +\beta_K x_K
$$



```{r}
wcgs.mult <- glm(chd69~age+chol+sbp+bmi+smoke, data=wcgs, family=binomial, subset=(chol<600))
```


```{r fit-mult, echo=FALSE, results='asis', fig.cap="Logistic regression of CHD on multiple covariates."}
tit <- knitr::opts_current$get("fig.cap")
tit_html <- paste0('<span id="tab:',
                   knitr::opts_current$get("label"),
                   '">(#tab:',
                   knitr::opts_current$get("label"),
                   ')</span>',
                   tit)
stargazer(wcgs.mult, type="html", digits=4,
          label = paste0("tab:", knitr::opts_current$get("label")),
          title = ifelse(knitr::is_latex_output(), tit, tit_html),
          notes="<span>&#42;</span>: p<0.1; <span>&#42;&#42;</span>: <strong>p<0.05</strong>; <span>&#42;&#42;&#42;</span>: p<0.01 <br> Standard errors in parentheses.",
          notes.append=F )
```



The coefficient ($0.635$) for the binary predictor variable `smoke` is the log odds ratio comparing smokers to nonsmokers for fixed values of `age`, `chol`, `sbp`, and `bmi`. The corresponding odds ratio
$$
\exp(0.635) = 1.89
$$
measures the proportionate increase in the odds of developing CHD for smokers compared to nonsmokers *adjusted for age, cholesterol, SBP, and BMI*.

Table \@ref(tab:log-mult) shows ddds ratios corresponding to all covariates and their confidence intervals.
```{r log-mult, echo=FALSE}
expcoef(wcgs.mult)[-1,] %>% 
    kable(digits = 4, escape=T, caption="Odds ratios with confidence intervals.") %>% 
    kable_styling(full_width = FALSE)
```


___

# Logistic Regression Model Representation

We have $n$ observations $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$.

Here $y_i$ is a binary outcome variable for observation $i$ and takes values 0 or 1, and $x_i$ is a predictor.

We want to model the probability of $y_i=1$ (observing a one) conditional on some predictors $x_i$:

$$
\begin{equation} (\#eq:linear-probability-model)
P\,(y_i=1 | x_i) = x_i'\beta .
\end{equation}
$$
We call Eq. \@ref(eq:linear-probability-model) as the <span style='color:#008B45FF'>Linear Probability Model</span>.


Note that $\mathbb{E}(y_i\vert x_i) = P\,(y_i=1 | x_i)$. That is, the conditional expectation of $y_i$ is the probability of observing a one given $x_i\,.$

Proof:

$$
\begin{aligned}
\mathbb{E}(y_i|x_i)  
&= \sum_{y_i\in \mathcal Y} y_i\, P\,(y=y_i|x_i) \\
&= 1 \times P\,(y_i=1|x_i) + 0 \times P\,(y_i=0|x_i) \\
&= P\,(y_i=1|x_i)
\end{aligned}
$$
where $\mathcal Y = \{0, 1\}$ is the sample space of $y_i$.

We can estimate $\beta$ in Eq. \@ref(eq:linear-probability-model) by OLS.

**Issue**: Will have $x'\beta$ outside of $[0,1]$ for some values of $x$. 

Alternative approach: let $F(·)$ be a known cdf and specify

$$
P\,(y_i=1|x_i) = F(x_i'\beta) \,.
$$

- When $\displaystyle F(u)= \frac{1}{1+e^{-u}}$ (the logistic cdf), where $u=x'\beta$, this is called the <span style='color:#008B45FF'>logit model</span>.
- When $\displaystyle F(u)= \Phi(u)$ (the standard normal cdf), this is called the <span style='color:#008B45FF'>probit model</span>.


Plug in $u=x'\beta$ and the logistic cdf, we can express the general logistic model as:

$$
\begin{align}
p(x) &= \frac{1}{1+e^{-u}}  (\#eq:logit-model0) \\
&= \frac{1}{1+e^{-x'\beta}} (\#eq:logit-model1) \\
&= \frac{e^{x'\beta}}{1+e^{x'\beta}}  (\#eq:logit-model2) \\
\end{align}
$$


On the left hand side, $p(x)=P(y=1\vert x)$ is a probability function that we observe a one ($y=1$ is the outcome) conditional on $x$ (predictors).

Eq. \@ref(eq:logit-model1) gives a “S-shaped” relationship between $p(x)$ and $x'\beta$ and ensures that $p(x)$ stays between 0 and 1. 
See Fig.&nbsp;\@ref(fig:logit-regression). No matter the value of $u$, $p$ remains bounded between 0 and 1.


```{r logit-regression, fig.cap="Logistic regression cdf", out.width = "80%"}
u <- seq(-4, 4, length.out = 100)
p <- 1/(1 + exp(-u))
plot(u, p, type = "l", ylab="F(u)")
```


- If $\mu=x'\beta=0$, then $p(x)=0.5$, there is an equal probability whether the outcome event will occur or not.
- If $\mu=x'\beta>0$, then $p(x)>0.5$, there is a larger probability that the outcome event will occur.
    - In this case, we could predict $E[y\vert x]=1$.
- If $\mu=x'\beta<0$, then $p(x)<0.5$, there is a smaller probability that the outcome event will occur.
    - In this case, we could predict $E[y\vert x]=0$.
- Note that it is *not* necessary to divide at $0.5$. Different threshold values could be used depending on the application in question.


Eq. \@ref(eq:logit-model1) can be written as:

$$
\begin{equation} (\#eq:logit-odds)
\frac{p(x)}{1-p(x)} = \exp{(x'\beta)}
\end{equation}
$$

We use an example of a single predictor with an intercept to demonstrate the **interpretation** of the regression coefficient $\beta$. 

If we consider two observations with covariate values $x$ and $x+\Delta x$, respectively.

The odds given $x$ can be obtained by:
$$
\text{odds}(x) = \frac{P(y=1\vert x)}{1- P(y=1\vert x)} = \exp(\beta_0 + \beta_1 x) \,.
$$
The odds given $(x+\Delta x)$ are
$$
\text{odds}(x+\Delta x) = \frac{P(y=1\vert x+\Delta x)}{1- P(y=1\vert x+\Delta x)} = \exp\left(\beta_0 + \beta_1 (x+\Delta x)\right)  \,.
$$

Their odds ratio becomes
$$
\begin{aligned}
\frac{\text{odds}(x+\Delta x)}{\text{odds}(x)} 
&= \frac{p(x+\Delta x)/[1-p(x+\Delta x)]}{p(x)/[1-p(x)}  \\
&= \frac{\exp{\left(\beta_0 + \beta_1 (x+\Delta x)\right)}}{\exp{\left(\beta_0 + \beta_1 x\right)}} \\
&= \exp{(\beta_1\Delta x)} \,.
\end{aligned}
$$
In particular, $e^{\beta_1}$ is the <span style='color:#337ab7'>odds ratio corresponding to one unit's increase in the value of the covariate</span>.

Suppose $x$ is binary (smoker vs. non-smoker), then 
$$
\frac{\text{odds}(x=1)}{\text{odds}(x=0)} = \exp (\beta_1). 
$$

In other words, the exponential of the regression coefficient $(e^{\beta_1})$ is the odds ratio for the odds given the treatment (smokers) to the odds in absence of the treatment (non-smokers).

In our case, the odds ratio for CHD for a smoker compared to a non-smoker based on the logistic regression in Table \@ref(tab:fit-smoke) is
$$
\frac{\text{odds}(smoke=1)}{\text{odds}(smoke=0)} = \exp (0.63) = 1.88 \,. 
$$


Suppose $x$ is numerical (e.g., age), then the interpretation would be changing $x$ by one unit would lead to the odds change by a factor of $\exp (\beta).$


- $\beta>0 \Rightarrow OR=\exp(\beta)>1$, indicating one unit change in $x$ would make the event more likely to happen.
- $\beta<0 \Rightarrow OR=\exp(\beta)<1$, indicating one unit change in $x$ would make the event less likely to happen.
- $\beta=0 \Rightarrow OR=\exp(\beta)=1$, indicating there is a $50/50$ chance that the event will occur given one unit change in $x$.


The same interpretations apply to logistic regressions with several indicators too, except that you have to add an additional comment — *holding all other variables constant*. 

By Eq. \@ref(eq:logit-odds), the logistic regression model may also be given as:

$$
\log \left[\frac{p(x)}{1-p(x)} \right] = x'\beta \,.
$$
Thus the logistic regression model is linear in the log-odds. 

___

## Confidence intervals for $\beta$ and odds ratio

A $95\%$ confidence interval for $\beta_j$, $j=1,\ldots,K,$ (based on the normal approximation) is given by:
$$
[\widehat{L}, \widehat{U}] = [\hat{\beta}_j - 1.96\cdot se(\hat{\beta}_j),\, \hat{\beta}_j + 1.96\cdot se(\hat{\beta}_j)]
$$
The odds ratio:
$$
OR = \exp (\hat{\beta}_j)
$$
We obtain the $95\%$ confidence interval for OR by taking the exponential of the lower and upper limits of the confidence interval for $\beta_j$
$$
[\exp(\widehat{L}),\, \exp(\widehat{U})] \,.
$$

Using the regression result in Table \@ref(tab:fit-age), we have $\hat{\beta}_1= 0.0744$ and $se(\hat{\beta}_1)=0.0113$. Hence, the $95\%$ CI for $\beta_1$:
$$
[0.0744-1.96\times 0.0113, \, 0.0744+1.96\times 0.0113] = [0.0523, \, 0.0965] \,.
$$
The estimate of odds ratio:
$$
OR = e^{0.0744} = 1.077
$$
The $95\%$ confidence interval for OR :
$$
[\exp(0.0523), \, \exp(0.0965)] = [1.0537, 1.1013]
$$

Note that odds ratios for continuous independent variables tend to be close to one, this does NOT suggest that the coefficients are insignificant. Use the *t*-test for statistical significance.

<span style='color:#008B45FF'>T-test</span> for $\beta_j=0$. 

$$
\begin{aligned}
&\text{H}_0: \beta_j=0  \\
&\text{H}_1: \beta_j\ne0 
\end{aligned}
$$

We use the following statistic
$$
z = \frac{\hat{\beta}_j-\beta_j}{se_{\hat{\beta}_j}}  \sim N(0,1) \,,
$$
which follows a standard normal distribution.

The *z*-value can be calculated as
$$
z = \frac{0.0744}{0.0113} = 6.58
$$
which is larger than the critical value at $5\%$ significance level
$$
c_{.025} = \Phi^{-1}(1-.025) = 1.96 \,.
$$

We conclude that $\beta_1$ is highly significant. That is, an increase in age significantly raises the odds for developing CHD.


___


## Estimation by Maximum Likelihood

If we condition on the $x_i$'s, the distribution of $y_i$ given $x_i$ is $\text{Bernoulli}\,(p(x))$, where $p(x)=F(x'\beta)$.

Conditional density is

$$
\begin{aligned}
f\,(y_i | x_i ) &= \begin{cases}
F(x'\beta) & \text{if $y_i=1$} \\
1- F(x'\beta) & \text{if $y_i=0$} \\
\end{cases} \\
&= F(x'\beta)^{y_i} \left[1- F(x'\beta)\right]^{1-y_i} \,.
\end{aligned}
$$


This gives the log likelihood

$$
\begin{align*}
\log L(\beta) &= \sum_{i=1}^n \log f\,(y_i | x_i ) \\
&= \sum_{i=1}^n \log \left\{F(x'\beta)^{y_i} \left[1- F(x'\beta)\right]^{1-y_i} \right\} \\
&= \sum_{i=1}^n \left\{y_i \log F(x'\beta) + {1-y_i} \log \left[1- F(x'\beta)\right] \right\} \\
&= \sum_{y_i=1} \log  F(x'\beta) + \sum_{y_i=0} \log \left[1- F(x'\beta)\right] \,.
\end{align*}
$$

$\hat{\beta}_{ML}$ maximizes $\log L(\beta)$.











