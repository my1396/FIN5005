---
title: "OLS -- Lecture"
author: "Menghan Yuan"
date: "Sept 11, 2024"
output: 
    bookdown::html_document2:
        mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML.js"
        self_contained: false
        toc: true
        toc_float: true
        toc_depth: 4
        number_sections: false
        css: "style.css"
---

<SCRIPT language="JavaScript" SRC="my_jxscript.js"></SCRIPT>

<a class="top-link" href="#top" id="js-top">↑</a>

```{r setup, include=F, echo=F}
library(knitr) # load packages
library(kableExtra)
library(tidyverse)
library(latex2exp)
library(stargazer)
library(bookdown)

# don't show code unless we explicitly set echo = TRUE
opts_chunk$set(echo = TRUE, message=FALSE, fig.align="center", fig.pos = "H")
opts <- options(knitr.kable.NA = "")

## control long outputs by using eg `max.lines = 10`
hook_output_default <- knitr::knit_hooks$get('output')
truncate_to_lines <- function(x, n) {
   if (!is.null(n)) {
      x = unlist(stringr::str_split(x, '\n'))
      if (length(x) > n) {
         # truncate the output
         x = c(head(x, n), '...\n')
      }
      x = paste(x, collapse = '\n') # paste first n lines together
   }
   x
}
knitr::knit_hooks$set(output = function(x, options) {
   max.lines <- options$max.lines
   x <- truncate_to_lines(x, max.lines)
   hook_output_default(x, options)
})
```


# Linear Regression Model


The multiple linear regression model is used to study the relationship between a <span style='color:#337ab7'>dependent variable</span> and one or more <span style='color:#337ab7'>independent variables</span>. The **generic form of the linear regression model** is

$$
\begin{equation} 
\begin{aligned}
y_i &= f(x_{1i}, x_{2i}, \ldots, x_{Ki}) + \varepsilon_i \\
&= x_{1i} \beta_1 + x_{2i} \beta_2 + \cdots + x_{Ki} \beta_K + \varepsilon_i
\end{aligned}(\#eq:scalar-form)
\end{equation}
$$
where 

- $y_i$ is the <span style='color:#337ab7'>dependent</span>, <span style='color:#337ab7'>response</span>, or <span style='color:#337ab7'>explained</span> variable for observation $i$;

- $x_{1i}, x_{2i}, \ldots, x_{Ki}$ are the <span style='color:#337ab7'>independent</span> or <span style='color:#337ab7'>explanatory variables</span>, also called the <span style='color:#337ab7'>regressors</span>, <span style='color:#337ab7'>predictors</span>, or <span style='color:#337ab7'>covariates</span>;

- $\beta_1, \beta_2, \ldots, \beta_K$ are the unknown parameters that we want to estimate;

and a sample of $n$ observations indexed $i = 1, 2, \ldots, n.$

Eq. \@ref(eq:scalar-form) is referred to as the scalar form.

We usually let $x_{1i}=1$ for all $i=1, 2, \ldots, n,$; where the parameter $\beta_1$ corresponds to the <span style='color:#008B45FF'>intercept</span>.


The error term $\varepsilon_i$ is a random disturbance, so named because it "disturbs" an otherwise stable relationship, hence also called as "disturbances." \
$\varepsilon_i$ reflects the combined effect of all additional influences on the outcome variable $y_i$. 

Several reasons why we need the error term $\varepsilon_i$:

- the effect of relevant variables not included in our set of $K$ explanatory variables (omitted variables); 
- any non-linear effects of our included variables $(x_{1i}, x_{2i}, \ldots, x_{Ki})$;
- measurement errors: For example, the difficulty of obtaining reasonable measures of incomes and expenditures. People might overstate incomes to make it look better.
- the factor is not observable. For instance, the aptitude or soft skills of people affect their income levels. 



**Notation**

<span style='color:#008B45FF'>The scalar form</span>
$$
\color{#008B45FF}{y_i = x_{1i} \beta_1 + x_{2i} \beta_2 + \cdots + x_{Ki} \beta_K + \varepsilon_i}
$$
can be written in <span style='color:#008B45FF'>vector form</span>:

$$
\color{#008B45FF}{y_i = \boldsymbol{x}_i'\boldsymbol{\beta} + \varepsilon_i}
$$
where $\boldsymbol{x}_i$ and $\boldsymbol{\beta}$ denote the $K\times 1$ column vectors:

$$
\boldsymbol{x}_i = \begin{pmatrix}
x_{1i} \\
x_{2i} \\
\vdots \\
x_{Ki}
\end{pmatrix}_{K\times 1} \quad \text{and} \quad
\boldsymbol{\beta} = \begin{pmatrix}
\beta_{1} \\
\beta_{2} \\
\vdots \\
\beta_{K}
\end{pmatrix}_{K\times 1}
$$
Sometimes you may see the $i$ subscripts omitted and the model written as
$$
y = \boldsymbol{x}'\boldsymbol{\beta} + \varepsilon
$$
for a typical observation.



We can also stack the observations for a sample of size $n$, and express the linear regression model in <span style='color:#008B45FF'>matrix form</span>:
$$
\color{#008B45FF}{\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}} ,
$$
where

$$
\boldsymbol{y} = \begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix}_{n\times 1},   \quad
\boldsymbol{X} = \begin{pmatrix}
x_1'\\
x_2'\\
\vdots \\
x_n'\\ 
\end{pmatrix}
= \begin{pmatrix}
x_{11} & x_{21} &  \cdots & x_{K1} \\
x_{12} & x_{22} & \cdots & x_{K2} \\
\vdots & \vdots & \ddots & \vdots \\
x_{1n} & x_{2n} & \cdots & x_{Kn} 
\end{pmatrix}_{n\times K},    \quad
\boldsymbol{\beta} = \begin{pmatrix}
\beta_{1} \\
\beta_{2} \\
\vdots \\
\beta_{K}
\end{pmatrix}_{K\times 1},   \quad \text{and }\;
\boldsymbol{\varepsilon} = \begin{pmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_n
\end{pmatrix}_{n\times 1}.
$$
$\boldsymbol{y}$ and $\boldsymbol{\varepsilon}$ are $n\times 1$ column vectors and $X$ is an $n\times K$ matrix.


Linear models often have an intercept term. We may have $x_{1i}=1,$ for all $i=1,\ldots, n$, in which case the parameter $\beta_1$ correspond to the intercept.

___

**A notational convention**

We use lower-case $\boldsymbol{x}$ to denote either a column or a row of $\boldsymbol{X}$, you should tell which it refers to from the context.

- We use subscripts $i$ and $t$ to denote row observations of $\boldsymbol{X}$, and
- subscripts $j$ and $k$ to denote columns (variables) of $\boldsymbol{X}$.


For instance, 
$$
y_i = \boldsymbol{x}_i'\boldsymbol{\beta} + \varepsilon_i
$$
$\boldsymbol{x}_i,$ $i=1,\ldots,n,$ denotes the $i^{\text{th}}$ <span style='color:#337ab7'>row</span> of $\boldsymbol{X}$, referring to a single observation $i$.

$$
\boldsymbol{x}_i = \begin{pmatrix}
x_{1i} \\
x_{2i} \\
\vdots \\
x_{Ki}
\end{pmatrix}_{K\times 1} \quad \text{and} \quad
X=\begin{pmatrix}
x_1' \\
x_2' \\
\vdots \\
x_n' \\
\end{pmatrix}_{n\times K}
$$

Sometimes we see the following form:
$$
\color{#008B45FF}{\boldsymbol{y} = \beta_1 \boldsymbol{x}_1 + \beta_2 \boldsymbol{x}_2 + \ldots + \beta_K \boldsymbol{x}_K + \varepsilon}
$$
Here $\boldsymbol{x}_k,$ $k=1,\ldots, K,$ denotes the $k^{\text{th}}$ <span style='color:#337ab7'>column</span> of $\boldsymbol{X}$, referring to a single variable $k$.

$$
\boldsymbol{x}_k = \begin{pmatrix}
x_{k1} \\
x_{k2} \\
\vdots \\
x_{kn}
\end{pmatrix}_{n\times 1} \quad \text{and} \quad
X = \begin{pmatrix}\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_K\end{pmatrix}_{n\times K}
$$

Note that we typically use **boldface** to indicate vectors and matrices, and regular typeface for scalars. \
But this convention is only loosely followed as it can be cumbersome for typing.

___


# Ordinary Least Squares (OLS)


The OLS estimator finds the parameter vector $\boldsymbol{\beta}$ which minimizes the sum of the squared errors, $\sum_{i=1}^n \varepsilon_i^2$.

Formally
$$
\hat{\boldsymbol{\beta}} = \arg\underset{\boldsymbol{\beta}}{\min} \sum_{i=1}^n \varepsilon_i^2
$$

In matrix form:
$$
\begin{aligned}
\hat{\boldsymbol{\beta}} &= \arg\underset{\boldsymbol{\beta}}{\min} \boldsymbol{\varepsilon}'\boldsymbol{\varepsilon} \\
&= \arg\underset{\boldsymbol{\beta}}{\min} (\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})'(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})
\end{aligned}
$$

To obtain the OLS estimator, we minimize $\phi(\boldsymbol{\beta})=\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}$ with respect to $\boldsymbol{\beta}$, where $\boldsymbol{\varepsilon}=\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}$.

Apply the first-order conditions:
$$
\begin{equation} (\#eq:normal-eqn)
\frac{\partial\phi(\beta)}{\partial\beta} = -2X'(y-X\beta) = 0
\end{equation}
$$
and the second-order conditions
$$
\frac{\partial^2\phi(\beta)}{\partial\beta\partial\beta'} = 2X'X >0,
$$
confirming that we obtain a unique minimum if $(X'X)^{-1}$ exists.

Eq \@ref(eq:normal-eqn) is referred to as the "normal equations."

The OLS estimator of $\beta$ is given by:
$$
\begin{equation} (\#eq:OLS-estimator)
\color{#EE0000FF}{\hat{\beta}_{\text{OLS}} = (X'X)^{-1}X'y} .
\end{equation}
$$
Note that

- $(X'X)^{-1}$ is a symmetric square $K \times K$ matrix.
- For $(X'X)^{-1}$ to exist, we need $X$ to be full rank. This is referred to as the full rank assumption. 


<div class = "boxed">
<em>Assumption:</em> $X$ is an $n\times K$ matrix with full rank $K$.
</div>

In plain language, <span style='color:#008B45FF'>full rank</span> means there are no exact linear relationships among the variables. \
This assumption is known as the <span style='color:#337ab7'>identification condition</span>.

$(X'X)^{-1}$ may not exist. If this is the case, then this matrix is called <span style='color:#337ab7'>non-invertible</span> or <span style='color:#337ab7'>singular</span> and is said to be of <span style='color:#337ab7'>less than full rank</span>. 

In practice, we calculate the determinant of a matrix to determine whether a matrix is invertible.

$$
\text{det}(A) \begin{cases}
\vert A \vert =0 \Longleftrightarrow A \text{ is singular ( or non-invertible), the inverse does not exist.} \\
\vert A \vert \ne 0 \Longleftrightarrow A \text{ is non-singular (or invertible), the inverse exists.} 
\end{cases}
$$

There are two possible reasons why this matrix might be non-invertible. 

- One is that $n < K,$ i.e., we have more independent variables than observations. This is unlikely to be a problem for us in practice. 
- The other is that one or more of the independent variables are a linear combination of the other variables i.e. *perfect* multicollinearity.


<div class = "boxed">

```{example, name="Short Rank"}
Suppose we have the following linear model:
    $$
    y=\beta_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \varepsilon,
    $$
where
    $$
    x_4 = x_2 + x_3 ,
    $$
meaning there is an exact linear dependency in the model.

Aim: We want to show that $\boldsymbol{\beta}' = (\beta_1, \beta_2, \beta_3, \beta_4)$ cannot be uniquely identified.

Now we let $\boldsymbol{\tilde{\beta}}' = (\beta_1, \beta_2+a, \beta_3+a, \beta_4-a)$ for any constant $a$. That is

$$
\begin{aligned}
\tilde{\beta}_1 &= \beta_1 \\
\tilde{\beta}_2 &= \beta_2+a \\
\tilde{\beta}_3 &= \beta_3+a \\
\tilde{\beta}_4 &= \beta_4-a \\
\end{aligned}
$$

Then we find that $\boldsymbol{x}'\boldsymbol{\beta} = \boldsymbol{x}'\boldsymbol{\tilde{\beta}}$. 

In other words, there is no way to estimate the parameters of this model.
```

</div>


The OLS estimator $\hat{\beta}_{\text{OLS}} = (X'X)^{-1}X'y$ can be written as:

$$
\begin{equation} (\#eq:simple-lm)
\hat{\beta}_{\text{OLS}} = \left(\sum_{i=1}^n x_i x_i' \right)^{-1} \left(\sum_{i=1}^n x_i y_i \right) .
\end{equation}
$$
In the <span style='color:#337ab7'>**simple linear regression**</span> case of an intercept and a single explanatory variable, i.e., $y_i=\beta_1+\beta_2x_{2i}+\varepsilon_i ,$ expanding eq \@ref(eq:simple-lm) gives that 


$$
\begin{aligned}
\hat{\beta}_1 &= \overline{y} - \hat{\beta}_2 \overline{x}_2 \\
\hat{\beta}_2 &= 
\frac{\left(\frac{1}{n}\sum_{i=1}^n x_{i2}y_i \right) - \left(\frac{1}{n}\sum_{i=1}^nx_{i2}\right) \left(\frac{1}{n}\sum_{i=1}^ny_{i}\right)}{\left(\frac{1}{n}\sum_{i=1}^n x_{i2}^2 \right) - \left(\frac{1}{n}\sum_{i=1}^n x_{i2}\right)^2} \\
&= \frac{\frac{1}{n}\sum_{i=1}^n (x_{2i}-\overline{x}_2)(y_i-\overline{y})}{\frac{1}{n}\sum_{i=1}^n (x_{2i}-\overline{x}_2)^2} \\
&= \frac{\text{Cov}(x_{2i}, y_i)}{\text{Var}(x_{2i})}
\end{aligned}
$$

That is, the estimated slope parameter, $\hat{\beta}_2$ is the ratio of the sample covaraicne between $x_{2i}$ and $y_i$ and the sample variance of $x_{2i}$.

- Note that this result does <span style='color:#EE0000FF'>**NOT**</span> generalize to models with more than one explanatory variable. ❌

- If you have more than one predictor variable, e.g., assume we have the following model
$$
y_i=\beta_1+\beta_2x_{2i}+\beta_3x_{3i}+\varepsilon_i ,
$$ 
where we have two explanatory variables $x_{2i}$ and $x_{3i}$. \
Since there might be correlation between $x_{2i}$ and $x_{3i} ,$ we **cannot** simply set $\displaystyle \hat{\beta}_2 = \frac{\text{Cov}(x_{2i}y_i)}{\text{Var}(x_{2i})}$ or $\displaystyle \hat{\beta}_3 = \frac{\text{Cov}(x_{3i}y_i)}{\text{Var}(x_{3i})}$. ❌
    - What one would like to do is to remove the linear influence of $x_2$ on $x_1$ and the other way around.
    - Refer to the <span style='color:#008B45FF'>Frisch-Waugh-Lovell (FWL) theorem</span> for more details if interested. We will not cover the theorem in our current course.
 
___

# Properties of OLS Estimators

The residuals for observations $i$ is $e_i=y_i-\hat{y}_i=y_i-x_i'\hat{\beta}_{\text{OLS}}$.

Note that we distinguish the error term $\varepsilon$ from residuals $e$.

- the error term $\varepsilon$ is from the population that cannot be observed;
- residuals $e$ are from samples that can be observed.

Observed value $y$ corresponds to the error term $\varepsilon$:
$$
y=X\beta + \varepsilon
$$

Predicted value $\hat{y}$ corresponds to residuals $e$:
$$
\begin{aligned}
\hat{y} &= X\hat{\beta} \\
y&=  X\hat{\beta} + e = \hat{y} + e
\end{aligned}
$$



We will introduce essential properties of OLS estimators. These properties do not depend on any assumptions -- they will always be true so long as we compute them by minimizing the sum of squared errors.


We plug in $\hat{\beta}$ to the normal equations:
$$
(X'X)\hat{\beta} = X'y
$$
Plug in $y=X\hat{\beta}+e$.
$$
\begin{split}
(X'X)\hat{\beta} &= X'(X\hat{\beta}+e) \\
(X'X)\hat{\beta} &= X'X\hat{\beta} + X'e \\
X'e &= \boldsymbol{0}
\end{split}
$$
or we can express in matrix form:
$$
\begin{pmatrix}
x_1' \\
x_2' \\
\vdots \\
x_K'
\end{pmatrix}_{K\times n} \cdot e_{n\times 1} =
\begin{pmatrix}
x_1'e \\
x_2'e \\
\vdots \\
x_K'e
\end{pmatrix}_{K\times 1} =
\begin{pmatrix}
0 \\
0 \\
\vdots \\
0
\end{pmatrix}_{K\times 1}
$$


Note that 

1. $\boldsymbol{x}_1'\boldsymbol{e}=0$, where $\boldsymbol{x}_1$ is a $n\times 1$ column vector of 1, indicating the sum of the residuals is zero, i.e., $\sum_{i=1}^n e_i =0$.

2. Following the last property, the sample average of residuals is zero. $\overline{e}=\frac{1}{n}\sum_{i=1}^n e_i=0.$

3. By construction, the OLS residuals $\boldsymbol{e}$ in our sample are <span style='color:#337ab7'>uncorrelated</span> with each of the $K$ explanatory variables included in the model.

    Uncorrelated means that $\text{Cov}(\boldsymbol{x}_k, \boldsymbol{e})=0$.

    $$
    \begin{align*}
    \text{Cov}(\boldsymbol{x}_k, \boldsymbol{e})  
    &= E\left\{\left[\boldsymbol{x}_k - E(\boldsymbol{x}_k)\right] \left[\boldsymbol{e} - E(\boldsymbol{e})\right]^T \right\} \\
    &= E[\boldsymbol{x}_k \boldsymbol{e}^T] - E[\boldsymbol{x}_k]E[\boldsymbol{e}]^T \\
    &= \boldsymbol{0} - E[\boldsymbol{x}_k] \boldsymbol{0}^T  \qquad (E[\boldsymbol{e}]=0 \text{ as } \boldsymbol{x}_1'\boldsymbol{e}=0) \\
    &= \boldsymbol{0}
    \end{align*} 
    $$


4. $\overline{\boldsymbol{y}}= \overline{\boldsymbol{X}}\hat{\boldsymbol{\beta}}$. This follows from the property $\overline{e}=0$.
$$
\boldsymbol{e} = \boldsymbol{y} - \boldsymbol{X}\hat{\boldsymbol{\beta}}
$$
    Sum all observations and divide by $n$:
    $$
    \overline{e} = \overline{y} - \overline{X}\hat{\beta}.
    $$
    
    Given $\overline{e}=0$, we have $\overline{y} = \overline{X}\hat{\beta}.$ That is, The regression hyperplane passes through the means of the observed values ($\overline{X}$ and $\overline{y}$).

5. The predicted values, $\hat{\boldsymbol{y}}$, are uncorrelated with the residuals $\boldsymbol{e}$.
$$
\begin{aligned}
\hat{\boldsymbol{y}}'\boldsymbol{e} &= (\boldsymbol{X}\hat{\boldsymbol{\beta}})'\boldsymbol{e} \\
&= \hat{\boldsymbol{\beta}}'\boldsymbol{X}'\boldsymbol{e} \\
&= \boldsymbol{0} \qquad (\boldsymbol{X}'\boldsymbol{e} = \boldsymbol{0})
\end{aligned}
$$
6. The mean of the predicted $y$’s for the sample will equal the mean of the observed $y$’s, i.e., $\overline{\hat{y}} = \overline{y}.$
$$
\begin{aligned}
\overline{y} &= \frac{1}{n} \sum_{i=1}^n y_i \\
&= \frac{1}{n} \sum_{i=1}^n (\hat{y}_i+e_i) \qquad (y_i=\hat{y}_i+e_i) \\
&= \frac{1}{n} \sum_{i=1}^n \hat{y}_i + \frac{1}{n} \sum_{i=1}^n e_i \qquad (\overline{e}=0) \\
&= \overline{\hat{y}}
\end{aligned}
$$

These properties always hold true. You should be careful not to infer anything from the residuals about the disturbances. 
For example, you cannot infer that the sum of the disturbances is zero or that the mean of the disturbances is zero just because this is true of the residuals -- this is true of the residuals just because we decided to minimize the sum of squared residuals.

___

# The Gauss-Markov Assumptions


Recall that $\hat{\beta}$ comes from our sample, but we want to learn about the true parameters $\beta$.
In order to do appropriate estimation and inference, we need a list of assumptions.

1. **Linearity** 

    $Y=X\beta+\varepsilon$ states that there is a linear relationship between $y$ and $X$.


2. **Full rank**
    
    $X$ is an $n\times K$ matrix of full rank. \
    This assumption states that there is no perfect multicollinearity. In other words, the columns
    of X are linearly independent. This assumption is known as the identification condition.

3. **Exogeneity of the independent variables**
    
    $E[\boldsymbol{\varepsilon}_i |\boldsymbol{X}] =0$ for $i=1,\ldots,n,$ or in vector form
    
    $$
    E[\boldsymbol{\varepsilon} |\boldsymbol{X}] = E\begin{bmatrix}
    \varepsilon_1 |\boldsymbol{X} \\
    \varepsilon_2 |\boldsymbol{X} \\
    \vdots \\
    \varepsilon_n |\boldsymbol{X} \\
    \end{bmatrix} = \boldsymbol{0}
    $$
    
    
    - $E[\boldsymbol{\varepsilon}_i |\boldsymbol{X}] =0$ means that the mean of each $\varepsilon_i$ conditional on all observations $\boldsymbol{x}_i$ is zero. \
    In plain language, no observations on $\boldsymbol{x}$ convey information about the expected value of the disturbance.

    > This is not likely to be true in a time-series setting. The observation at $t$, $x_t$, might provide information about $E[\varepsilon_{t+1}]$, that is $E[\varepsilon_{t+1} \vert x_t] \ne 0$. This is called <span style='color:#008B45FF'>autocorrelation</span>. 


    - The zero conditional mean implies that the unconditional mean is also zero. By the Law of Iterated Expectation (LIE),
    
    $$
    E[\boldsymbol{\varepsilon}] = E_\boldsymbol{X}\left[E[\boldsymbol{\varepsilon} \vert \boldsymbol{X}]\right] = E_\boldsymbol{X}[\boldsymbol{0}] = \boldsymbol{0}
    $$
    
    - Assumption 3 also implies that $\text{Cov}(\varepsilon_i|\boldsymbol{X})=0$ for all $i$.
    
    - $E[\boldsymbol{y}|\boldsymbol{X}] = \boldsymbol{X\beta}$. That is, $\boldsymbol{X\beta}$ is the conditional mean function.


    
4. **Homoskedasticity and no autocorrelation**

    $E[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'|X] = \sigma^2 \boldsymbol{I}$ or in matrix form 
    
    $$
    \begin{aligned}
    E[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'|X] &= 
    \begin{bmatrix} 
    E[\varepsilon_1^2 \vert \boldsymbol{X} ] & E[\varepsilon_1\varepsilon_2 \vert \boldsymbol{X}] & \cdots  & E[\varepsilon_1\varepsilon_n \vert \boldsymbol{X}] \\
    E[\varepsilon_2\varepsilon_1 \vert \boldsymbol{X} ] & E[\varepsilon_2^2 \vert \boldsymbol{X}] & \cdots  & E[\varepsilon_2\varepsilon_n \vert \boldsymbol{X}] \\
    \vdots & \vdots & \ddots & \vdots \\
    E[\varepsilon_n\varepsilon_1 \vert \boldsymbol{X} ] & E[\varepsilon_n\varepsilon_2 \vert \boldsymbol{X}] & \cdots  & E[\varepsilon_n^2 \vert \boldsymbol{X}] \\
    \end{bmatrix}  \\
    &= \begin{bmatrix}
    \sigma^2 & 0 & \cdots & 0 \\
    0 & \sigma^2 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots &\sigma^2 \\
    \end{bmatrix}
    \end{aligned}
    $$

    - Variance of the disturbances:
    $$
    \text{Var}[\varepsilon_i\vert \boldsymbol{X}] = \sigma^2, \qquad \text{ for all } i=1,\ldots,n.
    $$
    Constant variance is labeled as <span style='color:#008B45FF'>**homoskedasticity**</span>.
    
    - Covariance of the disturbances:
    $$
    \text{Cov}[\varepsilon_i, \varepsilon_j\vert \boldsymbol{X}] = 0,  \qquad \text{for all } i\ne j.
    $$
    Uncorrelatedness across observations is labeled as <span style='color:#008B45FF'>**nonautocorrelation**</span>.
    
    - We often denote the variacne-covariance matrix of the disturbances as $\Omega=E[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'|X]$.
    
    In case of homoskedasticity and nonautocorrelation, we have $\Omega=\sigma^2\boldsymbol{I}.$ And we call the disturbances <span style='color:#337ab7'>spherical disturbances</span>.
    
5. **Normal distribution of disturbances**

    $\boldsymbol{\varepsilon}\vert \boldsymbol{X} \sim N(\boldsymbol{0}, \sigma^2\boldsymbol{I})$. We assume that the disturbances are normally distributed, with zero mean and constant variance.
    
    - A useful implication of this assumption is that it implies that observations on $\varepsilon_i$ are statistically independent as well as uncorrelated.
    - Normality is not necessary to obtainmanyof the resultsweuse in multiple regression analysis, although it will enable us to obtain several exact statistical results. It does prove useful in constructing test statistics. Later, it will be possible to relax this assumption and retain most of the statistical results we obtain here. 

We call models satisfying assumptions 1--5 as the "<span style='color:#337ab7'>**Classical Linear Regression Model**</span>."

The Gauss-Markov Theorem states that, conditional on assumptions 1-5, there will be no other linear and unbiased estimator of the $\beta$ coefficients that has a smaller sampling variance. In other words, the OLS estimator is the <span style='color:#008B45FF'>**Best Linear, Unbiased and Efficient estimator (BLUE)**</span>.


___

## Mean and Variance of Least-Squres Estimator

Prove unbiasedness, $E(\hat{\boldsymbol{\beta}}|\boldsymbol{X}) = \boldsymbol{\beta}$.
$$
\begin{aligned}
\hat{\boldsymbol{\beta}} &= (X'X)^{-1}X'y \\
&= (X'X)^{-1}X'(X\boldsymbol{\beta}+\varepsilon) \\
&= (X'X)^{-1}X'X\boldsymbol{\beta} + (X'X)^{-1}(X'\varepsilon) \\
&= \boldsymbol{\beta} + (X'X)^{-1}X'\boldsymbol{\varepsilon}
\end{aligned}
$$
Taking the expectation with respect to $\hat{\beta}-\beta$.
$$
\begin{aligned}
E[\hat{\beta}-\beta \vert X] &= E[ (X'X)^{-1}(X'\boldsymbol{\varepsilon}) \vert X] \\
&= (X'X)^{-1}X'E[\boldsymbol{\varepsilon} \vert X] \\
&= \boldsymbol{0}.
\end{aligned}
$$


Variance of least-squares estimator:

$$
\begin{aligned}
\text{Var}(\boldsymbol{\hat{\beta}} \vert \boldsymbol{X}) &= E\left[(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})^T \vert \boldsymbol{X} \right] \\
&= E\left[(X'X)^{-1}X'\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}'X(X'X)^{-1} \vert \boldsymbol{X}\right] \\
&= (X'X)^{-1}X'\, E\left[\boldsymbol{\varepsilon\varepsilon}'\vert \boldsymbol{X}\right] \, X(X'X)^{-1} \\
&= (X'X)^{-1}X'\, \Omega \, X(X'X)^{-1}
\end{aligned}
$$
where $\Omega=E[\boldsymbol{\varepsilon\varepsilon}'\vert \boldsymbol{X}]$.

In case of the linear homoskedastic regression model, we have $E[\boldsymbol{\varepsilon\varepsilon}'\vert \boldsymbol{X}]=\sigma^2\boldsymbol{I}$. The variance matrix simplifies to 
$$
\boldsymbol{V}_{\hat{\boldsymbol{\beta}}} = \text{Var}(\hat{\boldsymbol{\beta}} \vert \boldsymbol{X}) = \sigma^2(\boldsymbol{X}'\boldsymbol{X})^{-1}
$$
We estimate $\sigma^2$ with $\hat{\sigma}^2$:
$$
\hat{\sigma}^2 = \frac{\sum_{i=1}^ne_i^2}{n-K}.
$$
What does the variance-covariance matrix of the OLS estimator look like?

$$
E[(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})^T \vert \boldsymbol{X}] = \begin{bmatrix}
\text{Var}(\hat{\beta}_1 \vert \boldsymbol{X}) & \text{Cov}(\hat{\beta}_1, \hat{\beta}_2 \vert \boldsymbol{X}) & \cdots & \text{Cov}(\hat{\beta}_1, \hat{\beta}_K \vert \boldsymbol{X})  \\
\text{Cov}(\hat{\beta}_1, \hat{\beta}_2 \vert \boldsymbol{X}) & \text{Var}(\hat{\beta}_2 \vert \boldsymbol{X}) & \cdots & \text{Cov}(\hat{\beta}_2, \hat{\beta}_K \vert \boldsymbol{X})  \\
\vdots & \vdots & \ddots & \vdots \\
\text{Cov}(\hat{\beta}_1, \hat{\beta}_K \vert \boldsymbol{X}) & \text{Cov}(\hat{\beta}_2, \hat{\beta}_K \boldsymbol{X}) & \cdots & \text{Var}(\hat{\beta}_K \vert \boldsymbol{X})  \\
\end{bmatrix}
$$
The standard errors of the $\hat{\beta}$ are given by the square root of the main diagonal of the variance-covariance matrix.

Under the assumption of normally distributed errors, $\boldsymbol{\varepsilon} \vert \boldsymbol{X} \sim N(\boldsymbol{0}, \sigma^2\boldsymbol{I})$, we have 

$$
\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta}, \sigma^2 (\boldsymbol{X}'\boldsymbol{X})^{-1}).
$$
Based on the distribution, we can conduct the hypothesis testing we are familiar with.







































