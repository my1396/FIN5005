---
title: "OLS -- Lecture"
author: "Menghan Yuan"
date: "Sept 11, 2024"
output: 
    bookdown::html_document2:
        mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML.js"
        self_contained: false
        toc: true
        toc_float: true
        toc_depth: 4
        number_sections: false
        css: "style.css"
---

<SCRIPT language="JavaScript" SRC="my_jxscript.js"></SCRIPT>

<a class="top-link" href="#top" id="js-top">â†‘</a>

```{r setup, include=F, echo=F}
library(knitr) # load packages
library(kableExtra)
library(tidyverse)
library(latex2exp)
library(stargazer)
library(bookdown)

# don't show code unless we explicitly set echo = TRUE
opts_chunk$set(echo = TRUE, message=FALSE, fig.align="center", fig.pos = "H")
opts <- options(knitr.kable.NA = "")

## control long outputs by using eg `max.lines = 10`
hook_output_default <- knitr::knit_hooks$get('output')
truncate_to_lines <- function(x, n) {
   if (!is.null(n)) {
      x = unlist(stringr::str_split(x, '\n'))
      if (length(x) > n) {
         # truncate the output
         x = c(head(x, n), '...\n')
      }
      x = paste(x, collapse = '\n') # paste first n lines together
   }
   x
}
knitr::knit_hooks$set(output = function(x, options) {
   max.lines <- options$max.lines
   x <- truncate_to_lines(x, max.lines)
   hook_output_default(x, options)
})
```


# Linear Regression Model


The multiple linear regression model is used to study the relationship between a <span style='color:#337ab7'>dependent variable</span> and one or more <span style='color:#337ab7'>independent variables</span>. The **generic form of the linear regression model** is

$$
\begin{equation} 
\begin{aligned}
y_i &= f(x_{1i}, x_{2i}, \ldots, x_{Ki}) + \varepsilon_i \\
&= x_{1i} \beta_1 + x_{2i} \beta_2 + \cdots + x_{Ki} \beta_K + \varepsilon_i
\end{aligned}(\#eq:scalar-form)
\end{equation}
$$
where 

- $y_i$ is the <span style='color:#337ab7'>dependent</span> or <span style='color:#337ab7'>explained</span> variable for observation $i$;
- $x_{1i}, x_{2i}, \ldots, x_{Ki}$ are the <span style='color:#337ab7'>independent</span> or <span style='color:#337ab7'>explanatory variables</span>, also called the <span style='color:#337ab7'>regressors</span>, <span style='color:#337ab7'>predictors</span>, or <span style='color:#337ab7'>covariates</span>.
- $\beta_1, \beta_2, \ldots, \beta_K$ are the unknown parameters that we want to estimate.

and a sample of $n$ observations indexed $i = 1, 2, \ldots, n.$

Eq. \@ref(eq:scalar-form) is referred to as the scalar form.

We usually let $x_{1i}=1$ for all $i=1, 2, \ldots, n,$; where the parameter $\beta_1$ corresponds to the <span style='color:#008B45FF'>intercept</span>.


The error term $\varepsilon_i$ is a random disturbance, so named because it "disturbs" an otherwise stable relationship, hence also called as "disturbances." \
$\varepsilon_i$ reflects the combined effect of all additional influences on the outcome variable $y_i$. 

Several reasons why we need the error term $\varepsilon_i$:

- the effect of relevant variables not included in our set of $K$ explanatory variables (omitted variables); 
- any non-linear effects of our included variables $(x_{1i}, x_{2i}, \ldots, x_{Ki})$;
- measurement errors: For example, the difficulty of obtaining reasonable measures of incomes and expenditures. People might overstate incomes to make it look better.
- the factor is not observable. For instance, the aptitude or soft skills of people affect their income levels. 



**Notation**

<span style='color:#008B45FF'>The scalar form</span>
$$
\color{#008B45FF}{y_i = x_{1i} \beta_1 + x_{2i} \beta_2 + \cdots + x_{Ki} \beta_K + \varepsilon_i}
$$
can be written in <span style='color:#008B45FF'>vector form</span>:

$$
\color{#008B45FF}{y_i = \boldsymbol{x}_i'\boldsymbol{\beta} + \varepsilon_i}
$$
where $\boldsymbol{x}_i$ and $\boldsymbol{\beta}$ denote the $K\times 1$ column vectors:

$$
\boldsymbol{x}_i = \begin{pmatrix}
x_{1i} \\
x_{2i} \\
\vdots \\
x_{Ki}
\end{pmatrix}_{K\times 1} \quad \text{and} \quad
\boldsymbol{\beta} = \begin{pmatrix}
\beta_{1} \\
\beta_{2} \\
\vdots \\
\beta_{K}
\end{pmatrix}_{K\times 1}
$$
Sometimes you may see the $i$ subscripts omitted and the model written as
$$
y = \boldsymbol{x}'\boldsymbol{\beta} + \varepsilon
$$
for a typical observation.



We can also stack the observations for a sample of size $n$, and express the linear regression model in <span style='color:#008B45FF'>matrix form</span>:
$$
\color{#008B45FF}{\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}} ,
$$
where

$$
\boldsymbol{y} = \begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix}_{n\times 1},   \quad
\boldsymbol{X} = \begin{pmatrix}
x_1'\\
x_2'\\
\vdots \\
x_n'\\ 
\end{pmatrix}
= \begin{pmatrix}
x_{11} & x_{21} &  \cdots & x_{K1} \\
x_{12} & x_{22} & \cdots & x_{K2} \\
\vdots & \vdots & \ddots & \vdots \\
x_{1n} & x_{2n} & \cdots & x_{Kn} 
\end{pmatrix}_{n\times K},    \quad
\boldsymbol{\beta} = \begin{pmatrix}
\beta_{1} \\
\beta_{2} \\
\vdots \\
\beta_{K}
\end{pmatrix}_{K\times 1},   \quad \text{and }\;
\varepsilon = \begin{pmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_n
\end{pmatrix}_{n\times 1}.
$$
$\boldsymbol{y}$ and $\boldsymbol{\varepsilon}$ are $n\times 1$ column vectors and $X$ is an $n\times K$ matrix.


Linear models often have an intercept term. We may have $x_{1i}=1,$ for all $i=1,\ldots, n,$, in which case the parameter $\beta_1$ correspond to the intercept.

___

**A notational convention**

We use lower-case $\boldsymbol{x}$ to denote either a column or a row of $\boldsymbol{X}$, you should tell which it refers to from the context.

- We use subscripts $i$ and $t$ to denote row observations of $\boldsymbol{X}$, and
- subscripts $j$ and $k$ to denote columns (variables) of $\boldsymbol{X}$.


For instance, 
$$
y_i = \boldsymbol{x}_i'\boldsymbol{\beta} + \varepsilon_i
$$
$\boldsymbol{x}_i,$ $i=1,\ldots,n,$ denotes the $i^{\text{th}}$ <span style='color:#337ab7'>row</span> of $\boldsymbol{X}$, referring to a single observation $i$.

$$
\boldsymbol{x}_i = \begin{pmatrix}
x_{1i} \\
x_{2i} \\
\vdots \\
x_{Ki}
\end{pmatrix}_{K\times 1} \quad \text{and} \quad
X=\begin{pmatrix}
x_1' \\
x_2' \\
\vdots \\
x_n' \\
\end{pmatrix}_{n\times K}
$$

Sometimes we see the following form:
$$
\color{#008B45FF}{\boldsymbol{y} = \beta_1 \boldsymbol{x}_1 + \beta_2 \boldsymbol{x}_2 + \ldots + \beta_K \boldsymbol{x}_K + \varepsilon}
$$
Here $\boldsymbol{x}_k,$ $k=1,\ldots, K,$ denotes the $k^{\text{th}}$ <span style='color:#337ab7'>column</span> of $\boldsymbol{X}$, referring to a single variable $k$.

$$
\boldsymbol{x}_k = \begin{pmatrix}
x_{k1} \\
x_{k2} \\
\vdots \\
x_{kn}
\end{pmatrix}_{n\times 1} \quad \text{and} \quad
X = \begin{pmatrix}x_1, x_2, \ldots, x_K\end{pmatrix}_{n\times K}
$$

Note that we typically use **boldface** to indicate vectors and matrices, and regular typeface for scalars. \
But this convention is only loosely followed as it can be cumbersome for typing.

___


# Ordinary Least Squares (OLS)


The OLS estimator finds the parameter vector $\boldsymbol{\beta}$ which minimizes the sum of the squared errors, $\sum_{i=1}^n \varepsilon_i^2$.

Formally
$$
\hat{\boldsymbol{\beta}} = \arg\underset{\boldsymbol{\beta}}{\min} \sum_{i=1}^n \varepsilon_i^2
$$

In matrix form:
$$
\begin{aligned}
\hat{\boldsymbol{\beta}} &= \arg\underset{\boldsymbol{\beta}}{\min} \boldsymbol{\varepsilon}'\boldsymbol{\varepsilon} \\
&= \arg\underset{\boldsymbol{\beta}}{\min} (\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})'(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})
\end{aligned}
$$

To obtain the OLS estimator, we minimize $\phi(\boldsymbol{\beta})=\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}$ with respect to $\boldsymbol{\beta}$, where $\boldsymbol{\varepsilon}=\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}$.

Apply the first-order conditions:
$$
\begin{equation} (\#eq:normal-eqn)
\frac{\partial\phi(\beta)}{\partial\beta} = -2X'(y-X\beta) = 0
\end{equation}
$$
and the second-order conditions
$$
\frac{\partial^2\phi(\beta)}{\partial\beta\partial\beta'} = 2X'X >0,
$$
confirming that we obtain a unique minimum if $(X'X)^{-1}$ exists.

\@ref(eq:normal-eqn) is referred to as the "normal equations."

The OLS estimator of $\beta$ is given by:
$$
\color{#EE0000FF}{\hat{\beta}_{\text{OLS}} = (X'X)^{-1}X'y} .
$$
Note that

- $(X'X)^{-1}$ is a symmetric square $K \times K$ matrix.
- For $(X'X)^{-1}$ to exist, we need $X$ to be full rank. This is referred to as the full rank assumption. 


<div class = "boxed">
<em>Assumption:</em> $X$ is an $n\times K$ matrix with full rank $K$.
</div>

In plain language, full rank means there are no exact linear relationships among the variables. \
This assumption is known as the <span style='color:#337ab7'>identification condition</span>.

$(X'X)^{-1}$ may not exist. If this is the case, then this matrix is called <span style='color:#337ab7'>non-invertible</span> or <span style='color:#337ab7'>singular</span> and is said to be of <span style='color:#337ab7'>less than full rank</span>. 

There are two possible reasons why this matrix might be non-invertible. 

- One, based on a trivial theorem about rank, is that $n < k$ i.e., we have more independent variables than observations. This is unlikely to be a problem for us in practice. 
- The other is that one or more of the independent variables are a linear combination of the other variables i.e. *perfect* multicollinearity.


```{example, name="Short Rank"}
Suppose we have the following linear model:
    $$
    y=\beta_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \varepsilon,
    $$
where
    $$
    x_4 = x_2 + x_3 ,
    $$
meaning there is an exact linear dependency in the model.

Aim: We want to show that $\boldsymbol{\beta}' = (\beta_1, \beta_2, \beta_3, \beta_4)$ cannot be uniquely identified.

Now we let $\boldsymbol{\tilde{\beta}}' = (\beta_1, \tilde{\beta_2}+a, \tilde{\beta_3}+a, \tilde{\beta_4}-a)$ for any constant $a$. 

Then we find that $\boldsymbol{x}'\boldsymbol{\beta} = \boldsymbol{x}'\boldsymbol{\tilde{\beta}}$. 

In other words, there is no way to estimate the parameters of this model.
```


# Properties of OLS Estimators

The residuals for observations $i$ is $e_i=y_i-\hat{y}_i=y_i-x_i'\hat{\beta}_{\text{OLS}}$.

Note that we distinguish the error term $\varepsilon$ from residuals $e$.

- the error term $\varepsilon$ is from the population that cannot be observed;
- residuals $e$ are from samples that can be observed.

We will introduce essential properties of OLS estimators. These properties do not depend on any assumptions -- they will always be true so long as we compute them by minimizing the sum of squared errors.


We plug in $\hat{\beta}$ to the normal equations:
$$
(X'X)\hat{\beta} = X'y
$$
Plug in $y=X\hat{\beta}+e$.
$$
\begin{split}
(X'X)\hat{\beta} &= X'(X\hat{\beta}+e) \\
(X'X)\hat{\beta} &= X'X\hat{\beta} + X'e \\
X'e &= \boldsymbol{0}
\end{split}
$$
or we can express in matrix form:
$$
\begin{pmatrix}
x_1' \\
x_2' \\
\vdots \\
x_K'
\end{pmatrix}_{K\times n} \cdot e_{n\times 1} =
\begin{pmatrix}
x_1'e \\
x_2'e \\
\vdots \\
x_K'e
\end{pmatrix}_{K\times 1} =
\begin{pmatrix}
0 \\
0 \\
\vdots \\
0
\end{pmatrix}_{K\times 1}
$$
This implies that, by construction, the OLS residuals $\boldsymbol{e}$ in our sample are <span style='color:#337ab7'>uncorrelated</span> with each of the $K$ explanatory variables included in the model.


```{proof}
Uncorrelated means that $\text{Cov}(x_k, e)=0$.

$$
\begin{align*}
\text{Cov}(x_k, e) &= E\left\{\left[x_k-E(x_k)\right] \left[e-E(e)\right]^T \right\} \\
&= E[x_ke^T] - E[x_k]E[e]^T \\
&= \boldsymbol{0} - E[x_k] \boldsymbol{0} \\
&= 0
\end{align*} 
$$
```





















































