---
title: "Hypothesis Testing -- Lecture"
author: "Menghan Yuan"
date: "Sept 11, 2024"
output: 
    bookdown::html_document2:
        mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML.js"
        self_contained: false
        toc: true
        toc_float: true
        toc_depth: 4
        number_sections: false
        css: "style.css"
---

<SCRIPT language="JavaScript" SRC="my_jxscript.js"></SCRIPT>

<a class="top-link" href="#top" id="js-top">↑</a>

```{r setup, include=F, echo=F}
library(knitr) # load packages
library(kableExtra)
library(tidyverse)
library(latex2exp)
library(stargazer)
library(bookdown)

# don't show code unless we explicitly set echo = TRUE
opts_chunk$set(echo = TRUE, message=FALSE, fig.align="center", fig.pos = "H")
opts <- options(knitr.kable.NA = "")

## control long outputs by using eg `max.lines = 10`
hook_output_default <- knitr::knit_hooks$get('output')
truncate_to_lines <- function(x, n) {
   if (!is.null(n)) {
      x = unlist(stringr::str_split(x, '\n'))
      if (length(x) > n) {
         # truncate the output
         x = c(head(x, n), '...\n')
      }
      x = paste(x, collapse = '\n') # paste first n lines together
   }
   x
}
knitr::knit_hooks$set(output = function(x, options) {
   max.lines <- options$max.lines
   x <- truncate_to_lines(x, max.lines)
   hook_output_default(x, options)
})
```


# Expectation and Variance of a Random Vector

- Now we have a $2\times 1$ random vector $X = (X_1, X_2)'$, we have
$$
\mathbb{E}(X) = 
\mathbb{E}\begin{pmatrix}
X_1 \\ X_2
\end{pmatrix}
= \begin{pmatrix}
\mathbb{E}(X_1) \\ \mathbb{E}(X_2)
\end{pmatrix}
$$

- The variance of the bivariate random vector is given by:
$$
\begin{aligned}
\text{Var}(X) &= \mathbb{E}\{(X-\mathbb{E}(X))(X-\mathbb{E}(X))'\} \\
&= \mathbb{E}\left\lbrace \begin{bmatrix} X_1-\mathbb{E}(X_1) \\
X_2-\mathbb{E}(X_2)
\end{bmatrix}
\begin{bmatrix} X_1-\mathbb{E}(X_1),
X_2-\mathbb{E}(X_2)
\end{bmatrix}
\right\rbrace \\
&= \mathbb{E}\begin{pmatrix}
[X_1-\mathbb{E}(X_1)]^2                    & [X_1-\mathbb{E}(X_1)][X_2-\mathbb{E}(X_2)]  \\
[X_2-\mathbb{E}(X_2)][X_1-\mathbb{E}(X_1)] & [X_2-\mathbb{E}(X_2)]^2
\end{pmatrix} \\
&= \begin{pmatrix}
\mathbb{E}\{[X_1-\mathbb{E}(X_1)]^2\}                    & \mathbb{E}\{[X_1-\mathbb{E}(X_1)][X_2-\mathbb{E}(X_2)]\}  \\
\mathbb{E}\{[X_2-\mathbb{E}(X_2)][X_1-\mathbb{E}(X_1)]\} & \mathbb{E}\{[X_2-\mathbb{E}(X_2)]^2\}
\end{pmatrix} \\
&= \begin{pmatrix}
\text{Var}(X_1)      & \text{Cov}(X_1, X_2) \\
\text{Cov}(X_1, X_2) & \text{Var}(X_2)
\end{pmatrix}
\end{aligned} 
$$
This is called the <span style='color:#008B45FF'>variance</span> (or also the <span style='color:#008B45FF'>covariance</span>) <span style='color:#008B45FF'>matrix</span>. 
This matrix is, by definition, symmetric.



___

**Linear Transformations of a Random Vector**

Let $X$ be a $p\times 1$ random vector, let $A$ be a non-random $q\times 1$ vector, and let $B$ be a non-random $p\times q$ matrix.

Then 
$$
Y = A + BX
$$
is a $q\times 1$ random vector.

- The expected value of this transformation is given by 
$$
\mathbb{E}(Y) = \mathbb{E}(A+BX) = A + B\mathbb{E}(X)
$$

- The variance of this transformation is given by
$$
\text{Var}(Y) = \text{Var}(A+BY) = \text{Var}(BY) = B\text{Var}(X)B'
$$

___

**Non-Negative Definiteness**

Remember that for a scalar random variable $X$, $\text{Var}(X)$ is a non-negative scalar.

For a $q\times 1$ random vector $Y$ (such as $Y=A+BX$ defined before), the variance $\text{Var}(Y)$ will be a $q \times q$ matrix. What is the counterpart of non-negativeness for matrices?

The appropriate concept is called <span style='color:#008B45FF'>non-negative definiteness</span> or <span style='color:#008B45FF'>positive semi-definiteness</span>.

Formal definition: A $q \times q$ square matrix $\Sigma$ is called non-negative definite (or positive semi-definite) if for any non-zero $q\times 1$ vector $a$ it holds that
$$
a'\Sigma a \ge 0.
$$
If the square matrix $\Sigma$ is non-negative definite, we write $\Sigma \ge 0.$


___

# Central Limit Theorem

Suppose that $X_1, \ldots, X_n$ is an independent and identically distributed (iid) sequence with a finite mean $\mathbb{E}(X_i)=\mu$ and variance  $\text{Var}(X_i)=\sigma^2$. 

Define a sample mean: $\overline{X}=\frac{1}{n}\sum_{i=1}^n X_i.$ It is also written as $\overline{X}_\color{#008B45FF}{n}$, we sometimes to write a subscript $n$ to denote the sample size. 

<span style='color:#337ab7'>Aim</span>: We would like to obtain a distributional approximation to  $\overline{X}.$

We have shown in the last lecture that in finite samples, $\mathbb{E}[\overline{X}]=\mu$ and $\text{Var}[\overline{X}]=\sigma^2/n$ (Refer to the expectation and variance of the sample mean).

Then, the <span style='color:#008B45FF'>**Lindeberg-Lévy CLT**</span> states that

$$
\frac{\overline{X}-\mathbb{E}[\overline{X}]}{\sqrt{\text{Var}[\overline{X}]}} 
= \frac{\overline{X}-\mu}{\sqrt{\sigma^2/n}} 
= \color{#008B45FF}{\sqrt{n}} \cdot \frac{\overline{X}-\mu}{\sigma} 
\xrightarrow{d} N(0,1)
$$

$\xrightarrow{d}$ means convergence in distribution.


CLT shows the simple process of <span style='color:#008B45FF'>averaging induces normality</span>.



Equivalently, we can write 

$$
\sqrt{n}\cdot\frac{\overline{X}-\mu}{\sigma} \overset{a}{\sim} N(0,1)
$$

$\overset{\rm a}{\sim}$ means "*approximately distributed with*." \
Or we can also write as


\begin{array}{rlrl}
	\sqrt{n} (\overline{X}-\mu)  &\xrightarrow{d} N(0,\sigma^2),  &
  \color{#008B45FF}{\sqrt{n} (\overline{X}-\mu)}  & \color{#008B45FF}{\overset{a}{\sim} N(0,\sigma^2)} \\
  
  \overline{X} -\mu  &\xrightarrow{d} N(0,\sigma^2/n), &
  \overline{X} -\mu  &\overset{a}{\sim} N(0,\sigma^2/n) \\

  \overline{X} &\xrightarrow{d} N(\mu,\sigma^2/n), &
  \color{#008B45FF}{\overline{X}} & \color{#008B45FF}{\overset{a}{\sim} N(\mu,\sigma^2/n)} .
\end{array} 


**Note**: The CLT is a very powerful result. $X_1, \ldots, X_n$ can be from <span style='color:#337ab7'>any possible distribution</span> (as long as it's *iid* with *finite mean and variance*), and still their <span style='color:#337ab7'>**normalized sample mean**</span> will be <span style='color:#008B45FF'>**standard normal**</span>.


- The scaling by $\color{#337ab7}{\sqrt{n}}$ is crucial.

- Once we suitably scale by $\sqrt{n}$, we can invoke the CLT and obtain that $\sqrt{n} (\overline{X}-\mu)$ or $\sqrt{n}\cdot\frac{\overline{X}-\mu}{\sigma}$ are asymptotically normal as $n\to\infty$.

- In practice, we replace $\sigma$ with $\widehat{\sigma}$ because we do not observe $\sigma$ but we do observe $\widehat{\sigma}$.

- Population variance estimators, $\widehat{\sigma}^2$. Two versions: 
  - <span style='color:#337ab7'>Biased</span> sample variance: 
  $$
  s_1^2 = \frac{1}{n} \sum_{i=1}^n [(X_i-\overline{X})^2] ,
  $$ and 
  - <span style='color:#337ab7'>Unbiased</span> sample variance: 
  $$
  s_2^2 = \frac{1}{n-1} \sum_{i=1}^n [(X_i-\overline{X})^2] .
  $$
  - Both options are consistent; using either is fine.


___

# Confidence Intervals

For an iid sequence $X_1,\ldots,X_n$ with finite mean and variance, we already know that
$$
\mathbb{E}(\overline{X}) = \mu \quad \text{and} \quad \text{Var}(\overline{X}) = \frac{\sigma^2}{n}.
$$
Intuitively, $\overline{X}$ is a very useful estimator of $\mu$ but, just like any estimator, it is subject to dispersion, given by $\frac{\sigma^2}{n}.$

Given a dataset, one typically reports the value of the estimator for that particular dataset. But this does not include information on the uncertainty due the estimator variance.

$\overline{X}$ itself is a point estimator for $\mu$. In order to measure the <span style='color:#337ab7'>estimation uncertainty</span>, we use the <span style='color:#008B45FF'>**confidence interval**</span>, which combines both the <span style='color:#337ab7'>value</span> of the estimator and the <span style='color:#337ab7'>variance</span> of that estimator.



**General notation**

To keep the discussion general, suppose that we have a quantity of interest $\theta$ and some estimator of it, $\widehat{\theta}$. Furthermore, suppose we already know that
$$
\frac{\widehat{\theta}-\theta}{\Sigma} \sim N(0,1).
$$
To refer to our sample average example, $\theta=\mu$, $\widehat{\theta}=\overline{X}$, and $\Sigma=\sigma/\sqrt{n}$.


Our goal is to obtain an interval which takes the form $[\widehat{L}, \widehat{U}]$, and the probability of the interval covers the true parameter value , i.e., $\mathbb{P}(\widehat{L}\le \theta \le \widehat{U})$, is high.


- $\widehat{L}$ is referred to as the lower bound, and $\widehat{U}$ as the upper bound.

-  $\mathbb{P}(\widehat{L}\le \theta \le \widehat{U})$ is often mis-interpreted as treating $\theta$ as random and $[\widehat{L}, \widehat{U}]$ as fixed. \
It is inappropriate to interpret $\mathbb{P}(\widehat{L}\le \theta \le \widehat{U})$ as the probability that $\theta$ lies within $[\widehat{L}, \widehat{U}]$.

- Instead, the correct interpretation is that the probability $\mathbb{P}(\widehat{L}\le \theta \le \widehat{U})$ treats the point $\theta$ as fixed and the interval $[\widehat{L}, \widehat{U}]$ as random. \
It is the probability that <span style='color:#337ab7'>the random interval covers the fixed true coefficient $\theta.$</span>

$[\widehat{L}, \widehat{U}]$ is called the $1-\alpha$ confidence interval when $\mathbb{P}(\widehat{L}\le \theta \le \widehat{U})=1-\alpha.$

- $\alpha$ is often known as the significance level. Common significance levels include $1\%$ and $5\%$.
- $1-\alpha$ is often known as the confidence level or the convergence probability. Common confidence levels are $99\%$ and $95\%$.


A good choice for a confidence interval is obtained by adding and subtracting from the estimator $\widehat{\theta}$ a fixed multiple of its standard error:
$$
[\widehat{L}, \widehat{U}] = [\widehat{\theta}-c\cdot s(\widehat{\theta}), \widehat{\theta}+c\cdot s(\widehat{\theta})]
$$
where $c>0$ is a pre-specified constant, often called the <span style='color:#008B45FF'>critical value</span>.

Given $\frac{\widehat{\theta}-\theta}{\Sigma} \sim N(0,1)$, we have
$$
\begin{equation} (\#eq:CI)
P\left(-c_{\alpha/2} < \frac{\widehat{\theta}-\theta}{\Sigma} < c_{\alpha/2}  \right) = 1-\alpha .
\end{equation}
$$
This requires $\Phi(c_{\alpha/2}) = 1-\frac{\alpha}{2}$. 

Hence, $c_{\alpha/2}=\Phi^{-1}(1-\frac{\alpha}{2})$, which is the $1-\frac{\alpha}{2}$ quantile of the standard normal distribution.  

Re-arranging \@ref(eq:CI), we have 
$$
P\left(\widehat{\theta}-c_{\alpha/2}\cdot \Sigma, \; \widehat{\theta}+c_{\alpha/2}\cdot \Sigma\right) = 1-\alpha
$$
This says the random interval, 
$$
(\widehat{\theta}-c_{\alpha/2}\cdot \Sigma, \; \widehat{\theta}+c_{\alpha/2}\cdot \Sigma),
$$
contains $\theta$ with probability $1-\alpha.$


One can also generate one-sided intervals:
$$
P\left(\widehat{\theta}-c_{\alpha}\cdot \Sigma < \theta \right) = 1-\alpha ,
$$
or
$$
P\left(\theta < \widehat{\theta}+c_{\alpha}\cdot \Sigma \right) = 1-\alpha ,
$$
but a two-sided interval is more common.

___

# Hypothesis Testing

Let us again consider the generic case of equation:

$$
\frac{\widehat{\theta}-\theta}{\Sigma} \sim N(0,1).
$$
Suppose we want to test the following claim:
$$
H_0: \theta = r \\
H_1: \theta \ne r,
$$
where $r$ is some scalar.

Here $H_0$ is the null hypothesis and $H_1$ is the alternative hypothesis.

- If $H_0: \theta = r$ is true, then plug in $r$ for $\theta$, we have
$$
\frac{\widehat{\theta}-r}{\Sigma} \sim N(0,1),
$$
and so $\frac{\widehat{\theta}-r}{\Sigma}$ should be close to zero, on average. So if $\frac{\widehat{\theta}-r}{\Sigma}$ is close to zero, we are inclined not to reject $H_0$.


- If $\frac{\widehat{\theta}-r}{\Sigma}$ is much greater or much less than zero, then we are inclined to reject $H_0$ and in favor of $H_1$.


In other words, we would be inclined to reject $H_0$ if
$$
\left\vert \frac{\widehat{\theta}-r}{\Sigma} \right\vert
$$
is much greater than zero.

Formally, our test statistic is
$$
\frac{\widehat{\theta}-r}{\Sigma} \sim N(0,1).
$$

we reject $H_0: \theta=r$ in favor of $H_1$ at $\alpha$ level of significance if 
$$
\left\vert \frac{\widehat{\theta}-r}{\Sigma} \right\vert > c_{\alpha/2}.
$$

Otherwise, we fail to reject $H_0$ (we never say ACCEPT $H_0$). 

We just say that we don't have enough evidence to reject $H_0$. This is equivalent to saying we don't have enough evidence to support the alternative hypothesis.

> The null hypothesis specifically means that no effect of some variable is found in the data. This is not the same thing as saying that “there is no effect.”
>
> A numeric example: If I have two ordinary 6-sided dice, the null hypothesis is that the dice are fair.
>
> - If I throw the dice once, and get two 2's, the null hypothesis has not been disproved, so I fail to reject it.  
> - If I throw the dice 6 times in a row and get two 2's each time, it doesn't look fair to me. I reject the null hypothesis because there is only a (1/36)^6 chance of that happening if the dice are fair.


Similarly, for one-sided hypothesis testing, we have

- Case 1: $H_0: \theta\ge r \quad \text{vs} \quad H_1: \theta<r$. We reject $H_0$ in favor of $H_1$ if 
$$
\frac{\widehat{\theta}-r}{\Sigma} < -c_{\alpha}.
$$
Otherwise, we fail to reject $H_0.$

- Case 2: $H_0: \theta\le r \quad \text{vs} \quad H_1: \theta>r$. We reject $H_0$ in favor of $H_1$ if 
$$
\frac{\widehat{\theta}-r}{\Sigma} > c_{\alpha}.
$$
Otherwise, we fail to reject $H_0.$


The critical values for typical levels of significance are:

- For one-sided tests,
    - 1\% level of significance : $c_\alpha = 2.33$,
    - 5\% level of significance : $c_\alpha = 1.64$, 
    - 10\% level of significance : $c_\alpha= 1.28$.

- For two-sided tests,
    - 1\% level of significance : $c_\alpha = 2.58$,
    - 5\% level of significance : $\color{#337ab7}{c_\alpha = 1.95}$, 
    - 10\% level of significance : $c_\alpha= 1.64$.



___

# Reading for this week

B. Hansen (2022), *Econometrics*, Princeton University Press, chaps 2-6.


W.H. Greene (2018), *Econometric Analysis*, 8th ed, Pearson Education, chaps 2-4.


If you want to brush up on your knowledge about matrix and basic probability theories, W.H. Greene (2018)'s appendix provides a good refresher.


___

# Reference

<https://qr.ae/p2CQZN>






