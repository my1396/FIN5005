<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Menghan Yuan" />


<title>Hypothesis Testing – Lecture</title>

<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Course Website</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Hypothesis Testing – Lecture</h1>
<h4 class="author">Menghan Yuan</h4>
<h4 class="date">Sept 11, 2024</h4>

</div>


<SCRIPT language="JavaScript" SRC="my_jxscript.js"></SCRIPT>
<p><a class="top-link" href="#top" id="js-top">↑</a></p>
<div id="expectation-and-variance-of-a-random-vector" class="section level1">
<h1>Expectation and Variance of a Random Vector</h1>
<ul>
<li><p>Now we have a <span class="math inline">\(2\times 1\)</span> random vector <span class="math inline">\(X = (X_1, X_2)&#39;\)</span>, we have
<span class="math display">\[
\mathbb{E}(X) =
\mathbb{E}\begin{pmatrix}
X_1 \\ X_2
\end{pmatrix}
= \begin{pmatrix}
\mathbb{E}(X_1) \\ \mathbb{E}(X_2)
\end{pmatrix}
\]</span></p></li>
<li><p>The variance of the bivariate random vector is given by:
<span class="math display">\[
\begin{aligned}
\text{Var}(X) &amp;= \mathbb{E}\{(X-\mathbb{E}(X))(X-\mathbb{E}(X))&#39;\} \\
&amp;= \mathbb{E}\left\lbrace \begin{bmatrix} X_1-\mathbb{E}(X_1) \\
X_2-\mathbb{E}(X_2)
\end{bmatrix}
\begin{bmatrix} X_1-\mathbb{E}(X_1),
X_2-\mathbb{E}(X_2)
\end{bmatrix}
\right\rbrace \\
&amp;= \mathbb{E}\begin{pmatrix}
[X_1-\mathbb{E}(X_1)]^2                    &amp; [X_1-\mathbb{E}(X_1)][X_2-\mathbb{E}(X_2)]  \\
[X_2-\mathbb{E}(X_2)][X_1-\mathbb{E}(X_1)] &amp; [X_2-\mathbb{E}(X_2)]^2
\end{pmatrix} \\
&amp;= \begin{pmatrix}
\mathbb{E}\{[X_1-\mathbb{E}(X_1)]^2\}                    &amp; \mathbb{E}\{[X_1-\mathbb{E}(X_1)][X_2-\mathbb{E}(X_2)]\}  \\
\mathbb{E}\{[X_2-\mathbb{E}(X_2)][X_1-\mathbb{E}(X_1)]\} &amp; \mathbb{E}\{[X_2-\mathbb{E}(X_2)]^2\}
\end{pmatrix} \\
&amp;= \begin{pmatrix}
\text{Var}(X_1)      &amp; \text{Cov}(X_1, X_2) \\
\text{Cov}(X_1, X_2) &amp; \text{Var}(X_2)
\end{pmatrix}
\end{aligned}
\]</span>
This is called the <span style="color:#008B45FF">variance</span> (or also the <span style="color:#008B45FF">covariance</span>) <span style="color:#008B45FF">matrix</span>.
This matrix is, by definition, symmetric.</p></li>
</ul>
<hr />
<p><strong>Linear Transformations of a Random Vector</strong></p>
<p>Let <span class="math inline">\(X\)</span> be a <span class="math inline">\(p\times 1\)</span> random vector, let <span class="math inline">\(A\)</span> be a non-random <span class="math inline">\(q\times 1\)</span> vector, and let <span class="math inline">\(B\)</span> be a non-random <span class="math inline">\(p\times q\)</span> matrix.</p>
<p>Then
<span class="math display">\[
Y = A + BX
\]</span>
is a <span class="math inline">\(q\times 1\)</span> random vector.</p>
<ul>
<li><p>The expected value of this transformation is given by
<span class="math display">\[
\mathbb{E}(Y) = \mathbb{E}(A+BX) = A + B\mathbb{E}(X)
\]</span></p></li>
<li><p>The variance of this transformation is given by
<span class="math display">\[
\text{Var}(Y) = \text{Var}(A+BY) = \text{Var}(BY) = B\text{Var}(X)B&#39;
\]</span></p></li>
</ul>
<hr />
<p><strong>Non-Negative Definiteness</strong></p>
<p>Remember that for a scalar random variable <span class="math inline">\(X\)</span>, <span class="math inline">\(\text{Var}(X)\)</span> is a non-negative scalar.</p>
<p>For a <span class="math inline">\(q\times 1\)</span> random vector <span class="math inline">\(Y\)</span> (such as <span class="math inline">\(Y=A+BX\)</span> defined before), the variance <span class="math inline">\(\text{Var}(Y)\)</span> will be a <span class="math inline">\(q \times q\)</span> matrix. What is the counterpart of non-negativeness for matrices?</p>
<p>The appropriate concept is called <span style="color:#008B45FF">non-negative definiteness</span> or <span style="color:#008B45FF">positive semi-definiteness</span>.</p>
<p>Formal definition: A <span class="math inline">\(q \times q\)</span> square matrix <span class="math inline">\(\Sigma\)</span> is called non-negative definite (or positive semi-definite) if for any non-zero <span class="math inline">\(q\times 1\)</span> vector <span class="math inline">\(a\)</span> it holds that
<span class="math display">\[
a&#39;\Sigma a \ge 0.
\]</span>
If the square matrix <span class="math inline">\(\Sigma\)</span> is non-negative definite, we write <span class="math inline">\(\Sigma \ge 0.\)</span></p>
<hr />
</div>
<div id="central-limit-theorem" class="section level1">
<h1>Central Limit Theorem</h1>
<p>Suppose that <span class="math inline">\(X_1, \ldots, X_n\)</span> is an independent and identically distributed (iid) sequence with a finite mean <span class="math inline">\(\mathbb{E}(X_i)=\mu\)</span> and variance <span class="math inline">\(\text{Var}(X_i)=\sigma^2\)</span>.</p>
<p>Define a sample mean: <span class="math inline">\(\overline{X}=\frac{1}{n}\sum_{i=1}^n X_i.\)</span> It is also written as <span class="math inline">\(\overline{X}_\color{#008B45FF}{n}\)</span>, we sometimes to write a subscript <span class="math inline">\(n\)</span> to denote the sample size.</p>
<p><span style="color:#337ab7">Aim</span>: We would like to obtain a distributional approximation to <span class="math inline">\(\overline{X}.\)</span></p>
<p>We have shown in the last lecture that in finite samples, <span class="math inline">\(\mathbb{E}[\overline{X}]=\mu\)</span> and <span class="math inline">\(\text{Var}[\overline{X}]=\sigma^2/n\)</span> (Refer to the expectation and variance of the sample mean).</p>
<p>Then, the <span style="color:#008B45FF"><strong>Lindeberg-Lévy CLT</strong></span> states that</p>
<p><span class="math display">\[
\frac{\overline{X}-\mathbb{E}[\overline{X}]}{\sqrt{\text{Var}[\overline{X}]}}
= \frac{\overline{X}-\mu}{\sqrt{\sigma^2/n}}
= \color{#008B45FF}{\sqrt{n}} \cdot \frac{\overline{X}-\mu}{\sigma}
\xrightarrow{d} N(0,1)
\]</span></p>
<p><span class="math inline">\(\xrightarrow{d}\)</span> means convergence in distribution.</p>
<p>CLT shows the simple process of <span style="color:#008B45FF">averaging induces normality</span>.</p>
<p>Equivalently, we can write</p>
<p><span class="math display">\[
\sqrt{n}\cdot\frac{\overline{X}-\mu}{\sigma} \overset{a}{\sim} N(0,1)
\]</span></p>
<p><span class="math inline">\(\overset{\rm a}{\sim}\)</span> means “<em>approximately distributed with</em>.”<br />
Or we can also write as</p>
<span class="math display">\[\begin{array}{rlrl}
    \sqrt{n} (\overline{X}-\mu)  &amp;\xrightarrow{d} N(0,\sigma^2),  &amp;
  \color{#008B45FF}{\sqrt{n} (\overline{X}-\mu)}  &amp; \color{#008B45FF}{\overset{a}{\sim} N(0,\sigma^2)} \\
  
  \overline{X} -\mu  &amp;\xrightarrow{d} N(0,\sigma^2/n), &amp;
  \overline{X} -\mu  &amp;\overset{a}{\sim} N(0,\sigma^2/n) \\

  \overline{X} &amp;\xrightarrow{d} N(\mu,\sigma^2/n), &amp;
  \color{red}{\overline{X}} &amp; \color{red}{\overset{a}{\sim} N(\mu,\sigma^2/n)} .
\end{array}\]</span>
<p><strong>Note</strong>: The CLT is a very powerful result. <span class="math inline">\(X_1, \ldots, X_n\)</span> can be from <span style="color:#337ab7">any possible distribution</span> (as long as it’s <em>iid</em> with <em>finite mean and variance</em>), and still their <span style="color:#337ab7"><strong>normalized sample mean</strong></span> will be <span style="color:#008B45FF"><strong>standard normal</strong></span>.</p>
<ul>
<li><p>The scaling by <span class="math inline">\(\color{#337ab7}{\sqrt{n}}\)</span> is crucial.</p></li>
<li><p>Once we suitably scale by <span class="math inline">\(\sqrt{n}\)</span>, we can invoke the CLT and obtain that <span class="math inline">\(\sqrt{n} (\overline{X}-\mu)\)</span> or <span class="math inline">\(\sqrt{n}\cdot\frac{\overline{X}-\mu}{\sigma}\)</span> are asymptotically normal as <span class="math inline">\(n\to\infty\)</span>.</p></li>
<li><p>In practice, we replace <span class="math inline">\(\sigma\)</span> with <span class="math inline">\(\widehat{\sigma}\)</span> because we do not observe <span class="math inline">\(\sigma\)</span> but we do observe <span class="math inline">\(\widehat{\sigma}\)</span>.</p></li>
<li><p>Population variance estimators, <span class="math inline">\(\widehat{\sigma}^2\)</span>. Two versions:</p>
<ul>
<li><span style="color:#337ab7">Biased</span> sample variance:
<span class="math display">\[
s_1^2 = \frac{1}{n} \sum_{i=1}^n [(X_i-\overline{X})^2] ,
\]</span> and</li>
<li><span style="color:#337ab7">Unbiased</span> sample variance:
<span class="math display">\[
s_2^2 = \frac{1}{n-1} \sum_{i=1}^n [(X_i-\overline{X})^2] .
\]</span></li>
<li>Both options are consistent; using either is fine.</li>
</ul></li>
</ul>
<hr />
<p>CLT Example: The amount of time customers spend in a grocery store is a random variable with mean <span class="math inline">\(\mu = 40\)</span> minutes and standard deviation <span class="math inline">\(\sigma = 15\)</span> minutes.</p>
<p><span class="math display">\[
\mathbb{E}[X] = \mu = 40, \quad \text{and} \quad \text{SD}(X) = \sigma = 15
\]</span>
Consider the following probabilities:</p>
<ul>
<li><p>Assuming <span class="math inline">\(X\)</span> is normally distributed, what is the probability that a randomly selected customer spends more than 42 minutes in the store, i.e., compute <span class="math inline">\(P(X &gt; 42)\)</span>?</p></li>
<li><p>Given a random sample of <span class="math inline">\(n=16\)</span> customers, what is the probability of the average time spent by the 64 customers exceeds 42 minutes, i.e., compute <span class="math inline">\(P(\overline{X}_{16} &gt; 42)\)</span>?</p></li>
<li><p>Given a random sample of <span class="math inline">\(n=64\)</span> customers, what is the probability of the average time spent by the 64 customers exceeds 42 minutes, i.e., compute <span class="math inline">\(P(\overline{X}_{64} &gt; 42)\)</span>?</p></li>
</ul>
<p><strong>Individual Customer</strong></p>
<p>We define the standard normal variable <span class="math inline">\(Z\)</span> as:</p>
<p><span class="math display">\[
Z = \frac{X - \mu}{\sigma}
\]</span>
To compute <span class="math inline">\(P(X &gt; 42)\)</span>, we convert to the standard normal:</p>
<p><span class="math display">\[
P(X &gt; 42) = P(\frac{X-40}{15} &gt; \frac{42-40}{15}) =  P(Z &gt; 0.1333) \approx 0.4469
\]</span></p>
<p>There is approximately a <strong>44.69%</strong> probability that a randomly selected customer will spend more than 42 minutes in the store.</p>
<p>Note that the <strong>specific distribution</strong> for <span class="math inline">\(X\)</span>, i.e., normality here, is <strong>required</strong> to compute the probability for a single customer.</p>
<hr />
<p><strong>Sample Mean for <span class="math inline">\(n = 16\)</span></strong></p>
<p><span class="math display">\[
\text{Standard Error} = \frac{15}{\sqrt{16}} = \frac{15}{4} = 3.75
\]</span></p>
<p><span class="math display">\[
Z = \frac{42 - 40}{3.75} = 0.533
\]</span></p>
<p><span class="math display">\[
P(\overline{X} &gt; 42) = P(Z &gt; 0.533) \approx 0.2970
\]</span>
A random sample of 16 customers has a <strong>29.70%</strong> chance of yielding an average time above 42 minutes.</p>
<hr />
<p><strong>Sample Mean for <span class="math inline">\(n = 64\)</span></strong></p>
<p>By the Central Limit Theorem, for large <span class="math inline">\(n\)</span>, the sampling distribution of the sample mean <span class="math inline">\(\overline{X}_{64}\)</span> is approximately normal (regardless the distribution of <span class="math inline">\(X\)</span>):</p>
<p><span class="math display">\[
\overline{X}_{64} \sim N (\mu, \frac{\sigma^2}{n}) = N\left(40, \frac{15^2}{64}\right) = N\left(40, \frac{225}{64}\right)
\]</span></p>
<p><span class="math display">\[
\text{Standard Error} = \frac{\sigma}{\sqrt{n}} = \frac{15}{\sqrt{64}} = 1.875
\]</span></p>
<p>We standardize using the standard normal variable <span class="math inline">\(Z\)</span>, defined as:</p>
<p><span class="math display">\[
Z = \frac{\overline{X} - \mu}{\sigma/\sqrt{n}} = \frac{42 - 40}{1.875} \approx 1.067
\]</span></p>
<p>Using the standard normal distribution table:</p>
<p><span class="math display">\[
P(\overline{X} &gt; 42) = P(Z &gt; 1.067) \approx 0.1436
\]</span></p>
<p>There is a <strong>14.35%</strong> probability that the average time spent by a random sample of 64 customers exceeds 42 minutes.</p>
<p><strong>Interpretation</strong></p>
<p>As sample size increases, the standard error (the standard deviation of the sample mean) decreases. This means the distribution of the sample mean becomes more concentrated around the population mean. Thus, extreme values (like sample means above 42) become less likely as <span class="math inline">\(n\)</span> increases.</p>
<p><img src="images/CLT.png" width="80%" style="display: block; margin: auto;" /></p>
<hr />
</div>
<div id="confidence-intervals" class="section level1">
<h1>Confidence Intervals</h1>
<p>For an iid sequence <span class="math inline">\(X_1,\ldots,X_n\)</span> with finite mean and variance, we already know that
<span class="math display">\[
\mathbb{E}(\overline{X}) = \mu \quad \text{and} \quad \text{Var}(\overline{X}) = \frac{\sigma^2}{n}.
\]</span>
Intuitively, <span class="math inline">\(\overline{X}\)</span> is a very useful estimator of <span class="math inline">\(\mu\)</span> but, just like any estimator, it is subject to dispersion, given by <span class="math inline">\(\frac{\sigma^2}{n}.\)</span></p>
<p>Given a dataset, one typically reports the value of the estimator for that particular dataset. But this does not include information on the uncertainty due the estimator variance.</p>
<p><span class="math inline">\(\overline{X}\)</span> itself is a point estimator for <span class="math inline">\(\mu\)</span>. In order to measure the <span style="color:#337ab7">estimation uncertainty</span>, we use the <span style="color:#008B45FF"><strong>confidence interval</strong></span>, which combines both the <span style="color:#337ab7">value</span> of the estimator and the <span style="color:#337ab7">variance</span> of that estimator.</p>
<p><strong>General notation</strong></p>
<p>To keep the discussion general, suppose that we have a quantity of interest <span class="math inline">\(\theta\)</span> and some estimator of it, <span class="math inline">\(\widehat{\theta}\)</span>. Furthermore, suppose we already know that
<span class="math display">\[
\frac{\widehat{\theta}-\theta}{\Sigma} \sim N(0,1).
\]</span>
To refer to our sample average example, <span class="math inline">\(\theta=\mu\)</span>, <span class="math inline">\(\widehat{\theta}=\overline{X}\)</span>, and <span class="math inline">\(\Sigma=\sigma/\sqrt{n}\)</span>.</p>
<p>Our goal is to obtain an interval which takes the form <span class="math inline">\([\widehat{L}, \widehat{U}]\)</span>, and the probability of the interval covers the true parameter value , i.e., <span class="math inline">\(\mathbb{P}(\widehat{L}\le \theta \le \widehat{U})\)</span>, is high.</p>
<ul>
<li><p><span class="math inline">\(\widehat{L}\)</span> is referred to as the lower bound, and <span class="math inline">\(\widehat{U}\)</span> as the upper bound.</p></li>
<li><p><span class="math inline">\(\mathbb{P}(\widehat{L}\le \theta \le \widehat{U})\)</span> is often mis-interpreted as treating <span class="math inline">\(\theta\)</span> (true unknown population parameter) as random and <span class="math inline">\([\widehat{L}, \widehat{U}]\)</span> as fixed.<br />
It is inappropriate to interpret <span class="math inline">\(\mathbb{P}(\widehat{L}\le \theta \le \widehat{U})\)</span> as the probability that <span class="math inline">\(\theta\)</span> lies within <span class="math inline">\([\widehat{L}, \widehat{U}]\)</span>.</p></li>
<li><p>Instead, the correct interpretation is that the probability <span class="math inline">\(\mathbb{P}(\widehat{L}\le \theta \le \widehat{U})\)</span> treats the point <span class="math inline">\(\theta\)</span> as fixed and the interval <span class="math inline">\([\widehat{L}, \widehat{U}]\)</span> as random.</p></li>
</ul>
<p><span class="math inline">\([\widehat{L}, \widehat{U}]\)</span> is called the <span class="math inline">\((1-\alpha)\)</span> confidence interval when <span class="math inline">\(\mathbb{P}(\widehat{L}\le \theta \le \widehat{U})=1-\alpha.\)</span></p>
<ul>
<li><span class="math inline">\(\alpha\)</span> is often known as the <span style="color:#008B45FF">significance level</span>. Common significance levels include <span class="math inline">\(1\%\)</span> and <span class="math inline">\(5\%\)</span>.</li>
<li><span class="math inline">\((1-\alpha)\)</span> is often known as the <span style="color:#008B45FF">confidence level</span> or the convergence probability. Common confidence levels are <span class="math inline">\(99\%\)</span> and <span class="math inline">\(95\%\)</span>.
<ul>
<li><span class="math inline">\((1-\alpha)\)</span> is the probability that <span style="color:#337ab7">the random interval covers the fixed true coefficient <span class="math inline">\(\theta.\)</span></span></li>
</ul></li>
</ul>
<p>A good choice for a confidence interval is obtained by adding and subtracting from the estimator <span class="math inline">\(\widehat{\theta}\)</span> a fixed multiple of its standard error:
<span class="math display">\[
[\widehat{L}, \widehat{U}] = [\widehat{\theta}-c\cdot s(\widehat{\theta}), \widehat{\theta}+c\cdot s(\widehat{\theta})]
\]</span>
where <span class="math inline">\(c&gt;0\)</span> is a pre-specified constant, often called the <span style="color:#008B45FF">critical value</span>.</p>
<p>Given <span class="math inline">\(\frac{\widehat{\theta}-\theta}{\Sigma} \sim N(0,1)\)</span>, we have
<span class="math display" id="eq:CI">\[
\begin{equation} \tag{1}
P\left(-c_{\alpha/2} &lt; \frac{\widehat{\theta}-\theta}{\Sigma} &lt; c_{\alpha/2}  \right) = 1-\alpha .
\end{equation}
\]</span>
This requires <span class="math inline">\(\Phi(c_{\alpha/2}) = 1-\frac{\alpha}{2}\)</span>.</p>
<p>Hence, <span class="math inline">\(c_{\alpha/2}=\Phi^{-1}(1-\frac{\alpha}{2})\)</span>, which is the <span class="math inline">\(\left(1-\frac{\alpha}{2}\right)\)</span> quantile of the standard normal distribution (inverse of the cdf of standard normal).</p>
<p>Re-arranging <a href="#eq:CI">(1)</a>, we have
<span class="math display">\[
P\left(\widehat{\theta}-c_{\alpha/2}\cdot \Sigma, \; \widehat{\theta}+c_{\alpha/2}\cdot \Sigma\right) = 1-\alpha
\]</span>
This says the random interval,
<span class="math display">\[
(\widehat{\theta}-c_{\alpha/2}\cdot \Sigma, \; \widehat{\theta}+c_{\alpha/2}\cdot \Sigma),
\]</span>
contains <span class="math inline">\(\theta\)</span> with probability <span class="math inline">\(1-\alpha.\)</span></p>
<p>One can also generate one-sided intervals:
<span class="math display">\[
P\left(\widehat{\theta}-c_{\alpha}\cdot \Sigma &lt; \theta \right) = 1-\alpha ,
\]</span>
or
<span class="math display">\[
P\left(\theta &lt; \widehat{\theta}+c_{\alpha}\cdot \Sigma \right) = 1-\alpha ,
\]</span>
where <span class="math inline">\(c_{\alpha}=\Phi^{-1}(1-\alpha)\)</span>, which is the <span class="math inline">\((1-\alpha)\)</span> quantile of the standard normal.
But a two-sided interval is more common.</p>
<hr />
</div>
<div id="hypothesis-testing" class="section level1">
<h1>Hypothesis Testing</h1>
<div id="two-sided-hypothesis-testing" class="section level2">
<h2>Two-sided hypothesis testing</h2>
<p>Let us again consider the generic case of equation:</p>
<p><span class="math display">\[
\frac{\widehat{\theta}-\theta}{\Sigma} \sim N(0,1).
\]</span>
Suppose we want to test the following claim:
<span class="math display">\[
H_0: \theta = r \\
H_1: \theta \ne r,
\]</span>
where <span class="math inline">\(r\)</span> is some scalar.</p>
<p>Here <span class="math inline">\(H_0\)</span> is the null hypothesis and <span class="math inline">\(H_1\)</span> is the alternative hypothesis.</p>
<ul>
<li><p>If <span class="math inline">\(H_0: \theta = r\)</span> is true, then plug in <span class="math inline">\(r\)</span> for <span class="math inline">\(\theta\)</span>, we have
<span class="math display">\[
\frac{\widehat{\theta}-r}{\Sigma} \sim N(0,1),
\]</span>
and so <span class="math inline">\(\frac{\widehat{\theta}-r}{\Sigma}\)</span> should be close to zero, on average. So if <span class="math inline">\(\frac{\widehat{\theta}-r}{\Sigma}\)</span> is close to zero, we are inclined not to reject <span class="math inline">\(H_0\)</span>.</p></li>
<li><p>If <span class="math inline">\(\frac{\widehat{\theta}-r}{\Sigma}\)</span> is much greater or much less than zero, then we are inclined to reject <span class="math inline">\(H_0\)</span> and in favor of <span class="math inline">\(H_1\)</span>.</p></li>
</ul>
<p>In other words, we would be inclined to reject <span class="math inline">\(H_0\)</span> if
<span class="math display">\[
\left\vert \frac{\widehat{\theta}-r}{\Sigma} \right\vert
\]</span>
is much greater than zero.</p>
<p>Formally, our <span style="color:#337ab7"><strong>test statistic</strong></span> is
<span class="math display">\[
Z = \frac{\widehat{\theta}-r}{\Sigma} \sim N(0,1).
\]</span></p>
<p><span style="color:#337ab7"><strong>Rejection rule</strong></span>:</p>
<ul>
<li><p>we reject <span class="math inline">\(H_0: \theta=r\)</span> in favor of <span class="math inline">\(H_1\)</span> at <span class="math inline">\(\alpha\)</span> level of significance if
<span class="math display">\[
  |Z| = \left\vert \frac{\widehat{\theta}-r}{\Sigma} \right\vert &gt; c_{\alpha/2},
  \]</span>
where the corresponding cut-off value <span class="math inline">\(c_{\alpha/2}\)</span> is called the <span style="color:#008B45FF">critical value</span>. <span class="math inline">\(c_{\alpha/2}\)</span> is defined as
<span class="math display">\[
  \begin{aligned}
  &amp; P(Z&gt;c_{\alpha/2}) = \alpha/2 , \\
  \Rightarrow \, &amp; P(Z\le c_{\alpha/2}) = 1-\alpha/2.
  \end{aligned}
  \]</span>
That is, <span class="math inline">\(c_{\alpha/2}\)</span> is the <span class="math inline">\((1-\alpha/2)\)</span> quantile (inverse cdf) of the distribution of <span class="math inline">\(Z\)</span>.</p></li>
<li><p>Otherwise, we <strong>fail to reject</strong> <span class="math inline">\(H_0\)</span> (we never say ACCEPT <span class="math inline">\(H_0\)</span>).</p>
<p>We just say that we don’t have enough evidence to reject <span class="math inline">\(H_0\)</span>. This is equivalent to saying we don’t have enough evidence to support the alternative hypothesis.</p>
<blockquote>
<p>The null hypothesis specifically means that no effect of some variable is found in the data. This is not the same thing as saying that “there is no effect.”</p>
<p>A numeric example: If I have two ordinary 6-sided dice, the null hypothesis is that the dice are fair.</p>
<ul>
<li>If I throw the dice once, and get two 2’s, the null hypothesis has not been disproved, so I fail to reject it.<br />
</li>
<li>If I throw the dice 6 times in a row and get two 2’s each time, it doesn’t look fair to me. I reject the null hypothesis because there is only a (1/36)^6 chance of that happening if the dice are fair.</li>
</ul>
</blockquote></li>
</ul>
<hr />
</div>
<div id="one-sided-hypothesis-testing" class="section level2">
<h2>One-sided hypothesis testing</h2>
<p>Similarly, for one-sided hypothesis testing, we have</p>
<ul>
<li><p>Case 1: <span class="math inline">\(H_0: \theta\ge r \quad \text{vs} \quad H_1: \theta&lt;r\)</span> (<span style="color:#008B45FF"><strong>left-tailed test</strong></span>).</p>
<p>We reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_1\)</span> if
<span class="math display">\[
  \frac{\widehat{\theta}-r}{\Sigma} &lt; -c_{\alpha} \; ,
  \]</span>
that is, the critical region is in the extreme left region (tail).
Otherwise, we fail to reject <span class="math inline">\(H_0.\)</span></p></li>
<li><p>Case 2: <span class="math inline">\(H_0: \theta\le r \quad \text{vs} \quad H_1: \theta&gt;r\)</span> (<span style="color:#008B45FF"><strong>right-tailed test</strong></span>).</p>
<p>We reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_1\)</span> if
<span class="math display">\[
  \frac{\widehat{\theta}-r}{\Sigma} &gt; c_{\alpha} \; ,
  \]</span>
that is, the critical region is in the extreme right region (tail).
Otherwise, we fail to reject <span class="math inline">\(H_0.\)</span></p></li>
</ul>
<p>The critical values for typical levels of significance are:</p>
<ul>
<li>For one-sided tests,
<ul>
<li><p>1% level of significance : <span class="math inline">\(c_{\alpha} = 2.33\)</span>,</p>
<pre class="r"><code>alpha &lt;- .01
qnorm(p=1-alpha)</code></pre>
<pre><code>## [1] 2.326348</code></pre>
<p><code>qnorm(p, mean=0, sd=1)</code> is the quantile function for the normal distribution, commonly used to find the quantile given a probability <code>p</code>. You can specify different values for the mean and standard deviation by adjusting the mean and sd parameters, respectively.</p></li>
<li><p>5% level of significance : <span class="math inline">\(c_{\alpha} = 1.64\)</span>,</p>
<pre class="r"><code>alpha &lt;- .05
qnorm(p=1-alpha)</code></pre>
<pre><code>## [1] 1.644854</code></pre></li>
<li><p>10% level of significance : <span class="math inline">\(c_{\alpha}= 1.28\)</span>.</p>
<pre class="r"><code>alpha &lt;- .1
qnorm(p=1-alpha)</code></pre>
<pre><code>## [1] 1.281552</code></pre></li>
</ul></li>
<li>For two-sided tests,
<ul>
<li><p>1% level of significance : <span class="math inline">\(c_{\alpha/2} = 2.58\)</span>,</p>
<pre class="r"><code>alpha &lt;- .01
qnorm(p=1-alpha/2)</code></pre>
<pre><code>## [1] 2.575829</code></pre></li>
<li><p>5% level of significance : <span class="math inline">\(\color{#337ab7}{c_{\alpha/2} = 1.96}\)</span>,</p>
<pre class="r"><code>alpha &lt;- .05
qnorm(p=1-alpha/2)</code></pre>
<pre><code>## [1] 1.959964</code></pre></li>
<li><p>10% level of significance : <span class="math inline">\(c_{\alpha/2}= 1.64\)</span>.</p>
<pre class="r"><code>alpha &lt;- .1
qnorm(p=1-alpha/2)</code></pre>
<pre><code>## [1] 1.644854</code></pre></li>
</ul></li>
</ul>
<hr />
</div>
</div>
<div id="reading-for-this-week" class="section level1">
<h1>Reading for this week</h1>
<p>B. Hansen (2022), <em>Econometrics</em>, Princeton University Press, chaps 2-6.</p>
<p>W.H. Greene (2018), <em>Econometric Analysis</em>, 8th ed, Pearson Education, chaps 2-4.</p>
<p>If you want to brush up on your knowledge about matrix and basic probability theories, W.H. Greene (2018)’s appendix provides a good refresher.</p>
<hr />
</div>
<div id="reference" class="section level1">
<h1>Reference</h1>
<p><a href="https://qr.ae/p2CQZN" class="uri">https://qr.ae/p2CQZN</a></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML.js";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
