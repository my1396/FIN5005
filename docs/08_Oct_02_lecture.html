<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Menghan Yuan" />


<title>Linear Regression Model – Lecture</title>

<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Course Website</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Linear Regression Model – Lecture</h1>
<h4 class="author">Menghan Yuan</h4>
<h4 class="date">Oct 2, 2024</h4>

</div>


<SCRIPT language="JavaScript" SRC="my_jxscript.js"></SCRIPT>
<p><a class="top-link" href="#top" id="js-top">↑</a></p>
<div id="one-way-anova-test" class="section level1">
<h1>One-way ANOVA test</h1>
<p>ANOVA stands for “analysis of variance”.
We use <span class="math inline">\(F\)</span>-test for one-way ANOVA to test whether the regression equation as a whole is significant.</p>
<div id="variance-decomposition" class="section level2">
<h2>Variance decomposition</h2>
<p>Consider the following model setting:</p>
<p><span class="math display">\[
y_i = x_{1i} \beta_1 + x_{2i} \beta_2 + \cdots + x_{Ki} \beta_K + \varepsilon_i\, ,
\]</span>
where <span class="math inline">\(\varepsilon_i \overset{iid}{\sim} N(0, \sigma^2)\)</span> and <span class="math inline">\(x_{1i}=1\)</span> for <span class="math inline">\(i=1,\ldots,n\)</span>.</p>
<ul>
<li><p>Total Sum of Squares (TSS)
<span class="math display">\[
\text{TSS} = \sum_{i=1}^n (y_i-\overline{y})^2,
\]</span>
where <span class="math inline">\(\overline{y}=\frac{1}{n} \sum_{i=1}^n y_i\)</span></p></li>
<li><p>Explained/Model Sum of Squares (ESS or MSS)
<span class="math display">\[
\text{ESS} = \sum_{i=1}^n (\hat{y}_i-\overline{y})^2
\]</span></p></li>
<li><p>Residual sum of squares
<span class="math display">\[
\text{RSS} = \sum_{i=1}^n (y_i-\hat{y}_i)^2
\]</span></p></li>
<li><p><span class="math inline">\(TSS = ESS + RSS\)</span>, that is
<span class="math display">\[
\sum_{i=1}^n (y_i-\overline{y})^2 = \sum_{i=1}^n (\hat{y}_i-\overline{y})^2 + \sum_{i=1}^n (y_i-\hat{y}_i)^2 .
\]</span>
Dividing both sides by <span class="math inline">\(\sum_{i=1}^n (y_i-\overline{y})^2\)</span> gives
<span class="math display">\[
  1 = \frac{\sum_{i=1}^n (\hat{y}_i-\overline{y})^2}{\sum_{i=1}^n (y_i-\overline{y})^2} - \frac{\sum_{i=1}^n (y_i-\hat{y}_i)^2}{\sum_{i=1}^n (y_i-\overline{y})^2} .
  \]</span>
Define <span style="color:#008B45FF"><span class="math inline">\(R^2=\frac{\sum_{i=1}^n (\hat{y}_i-\overline{y})^2}{\sum_{i=1}^n (y_i-\overline{y})^2}\)</span>, which is the ratio of <span class="math inline">\(ESS\)</span> to <span class="math inline">\(TSS\)</span></span>.
Rearranging the equation, we have
<span class="math display">\[
  \begin{aligned}
  R^2 &amp;= 1-\frac{\sum_{i=1}^n (y_i-\hat{y}_i)^2}{\sum_{i=1}^n (y_i-\overline{y})^2} \\
  &amp;= 1-\frac{RSS}{TSS} \\
  &amp;= \frac{ESS}{TSS}.
  \end{aligned}
  \]</span></p>
<p>Note that</p>
<ul>
<li><span class="math inline">\(R^2\)</span> is called “<span style="color:#008B45FF">the coefficient of determination</span>”.</li>
<li><span class="math inline">\(0\le R^2\le 1\)</span>, is a measure of goodness of fit.
<ul>
<li><span class="math inline">\(R^2=1\)</span> (perfit fit) occurs when <span class="math inline">\(y=\hat{y}\)</span>.</li>
<li><span style="color:#337ab7"><span class="math inline">\(R^2=0\)</span> occurs when there is only one intercept in the model</span>, i.e., <span class="math inline">\(y_i=\beta_1+\varepsilon\)</span>. The predicted value will be the sample average. <span class="math inline">\(\hat{y}_i=\overline{y}\)</span> for <span class="math inline">\(i=1,2,\ldots,n,\)</span> so that <span class="math inline">\(ESS = \sum_{i=1}^n (\hat{y}_i-\overline{y})^2=0.\)</span></li>
</ul></li>
<li><span class="math inline">\(R^2\)</span> is the square of the sample correlation coefficient between <span class="math inline">\(y_i\)</span> and <span class="math inline">\(\hat{y}_i\)</span>.</li>
<li><span class="math inline">\(R^2\)</span> measures the proportion of the total variation in the dependent variable
that is “accounted for” or “explained” by the model.</li>
</ul></li>
<li><p>Note that in a sample of size <span class="math inline">\(n\)</span>, we could always obtain a perfect fit (<span class="math inline">\(R^2 = 1\)</span>) simply by regressing <span class="math inline">\(y_i\)</span> on a set of <span class="math inline">\(n\)</span> linearly independent explanatory variables.</p>
<p>More generally, increasing the number of explanatory variables cannot reduce the fit as measured by <span class="math inline">\(R^2\)</span>.</p>
<p>Adding an explanatory variable that is irrelevant in the sample (s.t. the estimated coe¢cient on this variable is zero) leaves the fit unchanged.</p>
<p>While adding a variable that is not irrelevant in the sample (s.t. the estimated coefficient is not zero) improves the fit.</p>
<p>Some researchers prefer to report an “<span style="color:#008B45FF">adjusted <span class="math inline">\(R^2\)</span></span>” or “<span style="color:#008B45FF">R-bar-squared</span>”, defined by
<span class="math display">\[
  \overline{R}^2 = 1- \frac{n-1}{n-K}(1-R^2) ,
  \]</span>
which imposes a penalty as <span class="math inline">\(K\)</span> increases in a given sample size <span class="math inline">\(n\)</span>.</p></li>
<li><p>F-test for that all <span class="math inline">\(K-1\)</span> of the slope coefficients in a linear model are equal to zero, i.e., to test the exclusion of all explanatory variables except the intercept, <span class="math inline">\(\beta_1\)</span>.
Formally speaking.
<span class="math display">\[
\begin{aligned}
&amp;\text{H}_0: \beta_2=\beta_3=\cdots=\beta_K=0 \\
&amp;\text{H}_1: \text{At least one of the } \beta_2, \beta_3,\ldots, \beta_K,\text{is not zero.}
\end{aligned}
\]</span>
This is sometimes referred to as the <span style="color:#008B45FF">F-test</span> for <span style="color:#008B45FF">one-way ANOVA</span>.</p>
<p>The test statistic is given by:
<span class="math display">\[
  F = \left(\frac{n-K}{K-1}\right) \left(\frac{R^2}{1-R^2}\right) \sim F(K-1, n-K) .
  \]</span></p>
<p>where <span class="math inline">\(F\)</span> is <span class="math inline">\(F\)</span>-distributed with <span class="math inline">\(K-1\)</span> and <span class="math inline">\(n-K\)</span> degrees of freedom.</p>
<p>We reject <span class="math inline">\(\text{H}_0\)</span> if <span class="math inline">\(F&gt;F_{\alpha}(K-1,n-K)\)</span>.</p>
<ul>
<li><span class="math inline">\(F_{\alpha}(K-1,n-K)\)</span> is the <span class="math inline">\((1-\alpha)\)</span> percentile in the <span class="math inline">\(F(K-1, n-K)\)</span> distribution, corresponding to the level of significance, <span class="math inline">\(\alpha\)</span>.</li>
</ul>
<p><img src="https://drive.google.com/thumbnail?id=1LSF9a8en1SvwZDL76SHeZ0BpDYhHbLz-&sz=w1000" alt="F distribution" style="display: block; margin-right: auto; margin-left: auto; zoom:80%;" /></p>
<p>The <span class="math inline">\(p\)</span>-value is found by:
<span class="math display">\[
  \text{P-value} = \mathbb{P}(F&gt;F_{\text{obs}}) ,
  \]</span>
where <span class="math inline">\(F_{\text{obs}}\)</span> is your calcualted/observed test statistic based on your data sample.</p>
<ul>
<li><p>Large values of <span class="math inline">\(F\)</span> give evidence against the validity of the null hypothesis. Note that a large <span class="math inline">\(F\)</span> is induced by a large value of <span class="math inline">\(R^2\)</span>.</p></li>
<li><p>The logic of the test is that the <span class="math inline">\(F\)</span> statistic is a measure of the loss of fit (namely, all of <span class="math inline">\(R^2\)</span>) that results when we impose the restriction that all the slopes are zero. If <span class="math inline">\(F\)</span> is large, then the hypothesis is rejected.</p></li>
</ul></li>
</ul>
<hr />
</div>
</div>
<div id="t-test-for-the-effect-of-a-single-predictor" class="section level1">
<h1>T-test for the effect of a single predictor</h1>
<p>Quite often it is not very interesting to test the null hypothesis that none of the covariates have an effect.
<span style="color:#008B45FF">T-test</span> is used to test for if one specific covariate, <span class="math inline">\(\beta_j, j=1,\ldots,K,\)</span> has an effect.
<span class="math display">\[
\begin{aligned}
&amp;\text{H}_0: \beta_j=0 , \\
&amp;\text{H}_1: \beta_j\ne0 .
\end{aligned}
\]</span></p>
<p>We use the following statistic
<span class="math display">\[
t = \frac{\hat{\beta}_j-\beta_j}{se_{\hat{\beta}_j}}  \sim t(n-K) .
\]</span>
We reject <span class="math inline">\(\text{H}_0\)</span> if <span class="math inline">\(|t|&gt;c_{\alpha/2}\)</span>, where <span class="math inline">\(c_{\alpha/2}\)</span> is the <span class="math inline">\(\left(1-\frac{\alpha}{2}\right)\)</span> quantile of the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-K\)</span> degrees of freedom.</p>
<p>The two-sided <span class="math inline">\(p\)</span>-value is found by:
<span class="math display">\[
\begin{aligned}
p\text{-value} &amp;= 2\,\mathbb{P}(T&gt;|t|) \\
&amp;= 2\left(1-\mathbb{P}(T\le|t|)\right) ,
\end{aligned}
\]</span>
where <span class="math inline">\(T\)</span> is <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-K\)</span> degrees of freedom.</p>
<p>The <span class="math inline">\((1-\alpha)\)</span> confidence interval for <span class="math inline">\(\beta_j\)</span>:
<span class="math display">\[
[\hat{\beta}_j-c_{\alpha/2} \cdot se_{\hat{\beta}_j}, \hat{\beta}_j+c_{\alpha/2} \cdot se_{\hat{\beta}_j}] .
\]</span></p>
<p><strong>Note that</strong> the t-distribution approximates to a standard normal distribution when the degrees of freedom are high.
In practice, people often use the critical values of the standard normal distribution for simplification.</p>
<hr />
</div>
<div id="dummy-variables" class="section level1">
<h1>Dummy variables</h1>
<p>Dummy variables are useful to represent categorical predictors. For example, we usually set the variable <em>woman</em> as a dummy variable that only takes the values <span class="math inline">\(\{0,1\}\)</span>.
<span class="math display">\[
x_{1} = \begin{cases}
1 &amp; \text{ if sex=woman} \\
0 &amp; \text{ otherwise}
\end{cases} .
\]</span></p>
<ul>
<li>We call the level where <span class="math inline">\(x_1=0\)</span> as the <span style="color:#337ab7">reference level</span>.</li>
</ul>
<p>Given this notation we can formulate the regression model that involves a single dummy variable <span class="math inline">\(x_1\)</span>:
<span class="math display">\[
y_i = \beta_0 + \beta_1 x_{1,i}  + \varepsilon_i .
\]</span>
In this example,</p>
<ul>
<li><p>the intercept <span class="math inline">\(\beta_0\)</span> is equal to the conditional mean of <span class="math inline">\(y_i\)</span> for the <span class="math inline">\(x_{1,i}=0\)</span> subpopulation (men).</p></li>
<li><p>the slope <span class="math inline">\(\beta_1\)</span> is equal to the difference in the conditional means between <span class="math inline">\(x_{1,i}=1\)</span> (women) and <span class="math inline">\(x_{1,i}=0\)</span> (men).</p></li>
</ul>
<p><strong>More than two categories</strong></p>
<p>When there are several categories, a set of binary variables is necessary. For instance, a quarter variable can take four categories.</p>
<p><span class="math display">\[
quarter =
\begin{cases}
\text{1st quarter} \\
\text{2nd quarter} \\
\text{3rd quarter} \\
\text{4th quarter}
\end{cases}
\]</span>
Let’s write a consumption function for quarterly data as
<span class="math display">\[
y_t = \beta_1 + \beta_2 x_t + \delta_1 D_{t1} + \delta_{2} D_{t2} + \delta_3 D_{t3} + \varepsilon_t ,
\]</span>
where <span class="math inline">\(x_t\)</span> is disposable income, and</p>
<span class="math display">\[\begin{aligned}
D_{t1} &amp;= \begin{cases}
1 &amp; t \text{ is in the 1st quarter} \\
0 &amp; \text{otherwise} \\
\end{cases} \\

D_{t2} &amp;= \begin{cases}
1 &amp; t \text{ is in the 2nd quarter} \\
0 &amp; \text{otherwise} \\
\end{cases} \; . \\

D_{t3} &amp;= \begin{cases}
1 &amp; t \text{ is in the 3rd quarter} \\
0 &amp; \text{otherwise} \\
\end{cases}
\end{aligned}\]</span>
<p>Here we used 4th quarter as the base period. Any of the four quarters can be used as the base period.</p>
<p>Note that only three of the four quarterly dummy variables are included in the model. If the fourth were included, then the four dummy variables would sum to one at every observation, which would reproduce the constant term—a case of perfect multicollinearity.</p>
<hr />
<div id="bone-density-example" class="section level2">
<h2>Bone density example</h2>
<p>We considered a study of bone mineral density (in g/cm<span class="math inline">\(^2\)</span>) for rats given isoflavone and for rats in a control group. We want to test if isoflavone have an effect on bone mineral density.</p>
<pre class="r"><code># load dataset
bonedensity &lt;- read_csv(&quot;https://raw.githubusercontent.com/my1396/course_dataset/refs/heads/main/bonedensity.csv&quot;)
bonedensity</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["density"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["group"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.228","2":"1"},{"1":"0.207","2":"1"},{"1":"0.234","2":"1"},{"1":"0.220","2":"1"},{"1":"0.217","2":"1"},{"1":"0.228","2":"1"},{"1":"0.209","2":"1"},{"1":"0.221","2":"1"},{"1":"0.204","2":"1"},{"1":"0.220","2":"1"},{"1":"0.203","2":"1"},{"1":"0.219","2":"1"},{"1":"0.218","2":"1"},{"1":"0.245","2":"1"},{"1":"0.210","2":"1"},{"1":"0.250","2":"2"},{"1":"0.237","2":"2"},{"1":"0.217","2":"2"},{"1":"0.206","2":"2"},{"1":"0.247","2":"2"},{"1":"0.228","2":"2"},{"1":"0.245","2":"2"},{"1":"0.232","2":"2"},{"1":"0.267","2":"2"},{"1":"0.261","2":"2"},{"1":"0.221","2":"2"},{"1":"0.219","2":"2"},{"1":"0.232","2":"2"},{"1":"0.209","2":"2"},{"1":"0.255","2":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>We formulate as the following equation:
<span class="math display">\[
y = \beta_0 + \beta_1x_1 + \varepsilon ,
\]</span>
where <span class="math inline">\(x_1\)</span> is a dummy variable indicating control/treatment groups.
<span class="math display">\[
x_1 = \begin{cases}
1 &amp; \text{ if group=2 (treatment)} \\
0 &amp; \text{ if group=1 (control/reference)}
\end{cases} .
\]</span>
Let <span class="math inline">\(\mu_1\)</span> be the expected outcome in the reference group and <span class="math inline">\(\mu_2\)</span> be the expected outcome in the treatment group. That is
<span class="math display">\[
E(y|x_1) = \begin{cases}
\mu_0 &amp; \text{ if group=1} \\
\mu_1 &amp; \text{ if group=2} \\
\end{cases} .
\]</span>
This indicates
<span class="math display">\[
\begin{aligned}
\beta_0 &amp;= \mu_0\, , \\
\beta_1 &amp;= \mu_1-\mu_0 \,.
\end{aligned}
\]</span></p>
<p>Run the regression.</p>
<pre class="r"><code># define group to be a categorical covariate (factor)
bonedensity &lt;- bonedensity %&gt;% mutate(group = factor(group))
bonedensity</code></pre>
<pre><code>## # A tibble: 30 × 2
##    density group
##      &lt;dbl&gt; &lt;fct&gt;
##  1   0.228 1    
##  2   0.207 1    
##  3   0.234 1    
##  4   0.22  1    
##  5   0.217 1    
##  6   0.228 1    
##  7   0.209 1    
##  8   0.221 1    
##  9   0.204 1    
## 10   0.22  1    
## # ℹ 20 more rows</code></pre>
<pre class="r"><code>lm.density &lt;- lm(density~group,data=bonedensity)
summary(lm.density)</code></pre>
<pre><code>## 
## Call:
## lm(formula = density ~ group, data = bonedensity)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.029067 -0.011367 -0.000367  0.009733  0.031933 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.218867   0.004027  54.343  &lt; 2e-16 ***
## group2      0.016200   0.005696   2.844  0.00823 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.0156 on 28 degrees of freedom
## Multiple R-squared:  0.2242, Adjusted R-squared:  0.1964 
## F-statistic:  8.09 on 1 and 28 DF,  p-value: 0.008227</code></pre>
<p>Summarize the output in a regression table.</p>
<table style="text-align:center">
<caption>
<strong>Regression Results for bone density</strong>
</caption>
<tr>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
<em>Dependent variable:</em>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
density
</td>
</tr>
<tr>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
group2
</td>
<td>
0.016<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.006)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Constant
</td>
<td>
0.219<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.004)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
Observations
</td>
<td>
30
</td>
</tr>
<tr>
<td style="text-align:left">
R<sup>2</sup>
</td>
<td>
0.224
</td>
</tr>
<tr>
<td style="text-align:left">
Adjusted R<sup>2</sup>
</td>
<td>
0.196
</td>
</tr>
<tr>
<td style="text-align:left">
Residual Std. Error
</td>
<td>
0.016 (df = 28)
</td>
</tr>
<tr>
<td style="text-align:left">
F Statistic
</td>
<td>
8.090<sup>***</sup> (df = 1; 28)
</td>
</tr>
<tr>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
<em>Note:</em>
</td>
<td style="text-align:right">
<span>***</span>: p&lt;0.01; <span>**</span>: <strong>p&lt;0.05</strong>; <span>*</span>: p&lt;0.1
</td>
</tr>
</table>
<p> </p>
<ul>
<li>The intercept (0.219) is the mean bone density in group 1 (the reference group).</li>
<li>The slope estimate for group2 (0.016) is the difference between the means of bone density in the two groups.
<ul>
<li>The <span class="math inline">\(t\)</span>-value (2.844) and p-value (0.00823) equals the <span class="math inline">\(t\)</span>-test. Given that the p-value is smaller than 5%, we conclude that the treatment of isoflavone has significant effect on the bone density of rats.</li>
</ul></li>
</ul>
<p>Visualize the distribution of the two groups – <strong>Box plots</strong>.</p>
<p>Box plot statistics:</p>
<ul>
<li>extreme of the lower whisker: minimum</li>
<li>the lower hinge: first quartile or 25th percentile</li>
<li>the median</li>
<li>the upper hinge: third quartile or 75th percentile, and</li>
<li>the extreme of the upper whisker: maximum</li>
</ul>
<pre class="r"><code>boxplot(density~group, data=bonedensity)</code></pre>
<p><img src="08_Oct_02_lecture_files/figure-html/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;" /></p>
<hr />
</div>
</div>
<div id="t-test-for-equal-population-means" class="section level1">
<h1>T-test for equal population means</h1>
<p>We would like to determine a confidence interval for the treatment effect and test if the difference is statistically significant – <span style="color:#008B45FF"><span class="math inline">\(t\)</span>-test for equal population means</span>.</p>
<pre class="r"><code>t.test(density~group, data=bonedensity, var.equal=T)</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  density by group
## t = -2.8442, df = 28, p-value = 0.008227
## alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0
## 95 percent confidence interval:
##  -0.027867161 -0.004532839
## sample estimates:
## mean in group 1 mean in group 2 
##       0.2188667       0.2350667</code></pre>
<p><strong>Mathematical formulations</strong></p>
<p>The two-sample <em>t</em>-test is used to test for the difference in two population means, <span class="math inline">\(\mu_1-\mu_2\)</span>.</p>
<p>Suppose we wish to compare the means of two distinct populations. Figure <a href="#fig:t-test-diagram">1</a> illustrates the conceptual framework of our investigation in this and the next section. Each population has a mean (<span class="math inline">\(\mu\)</span>) and a standard deviation (<span class="math inline">\(\sigma\)</span>).</p>
<p>We draw a random sample from Population 1 and label the sample statistics it yields with the subscript 1. Particularly, we have sample mean <span class="math inline">\((\overline{x}_1)\)</span> and sample standard deviation <span class="math inline">\((s_1)\)</span>.</p>
<p>We draw a sample from Population 2 and label its sample statistics with the subscript 2. Particularly, we have sample mean <span class="math inline">\((\overline{x}_1)\)</span> and sample standard deviation <span class="math inline">\((s_1)\)</span>.</p>
<p>Our goal is to use the information in the samples to estimate the difference in the means of the two populations, <span class="math inline">\(\mu_1-\mu_2\)</span>, and to make statistically valid inferences about it.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:t-test-diagram"></span>
<img src="images/equal%20sample%20mean.jpg" alt="T-test of Two Population Means" width="40%" />
<p class="caption">
Fig. 1: T-test of Two Population Means
</p>
</div>
<hr />
<div id="equal-population-variances" class="section level2">
<h2>Equal Population Variances</h2>
<p>Suppose that the data for the two independent groups are random samples from <span class="math inline">\(N(\mu_1, \sigma_1^2)\)</span> and <span class="math inline">\(N(\mu_2, \sigma_2^2)\)</span>, respectively. Here we assume <strong>equal population variances</strong>, <span class="math inline">\(\sigma_1^2=\sigma_2^2\)</span>.</p>
<p>The observations are denotes:</p>
<ul>
<li>group 1: <span class="math inline">\(x_{11}, x_{12}, \ldots, x_{1,n_1}\)</span></li>
<li>group 2: <span class="math inline">\(x_{21}, x_{22}, \ldots, x_{2,n_2}\)</span></li>
</ul>
<p>We have all together <span class="math inline">\(n=n_1+n_2\)</span> observations.</p>
<p><span class="math display">\[
\text{H}_0: \mu_1 - \mu_2 =0  \quad \text{(or equivalently } H_0:  \mu_1 = \mu_2)
\]</span>
Against any of the alternative hypotheses:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \text{Two-tailed: } &amp;&amp; \text{H}_1: \mu_1 - \mu_2 \ne 0, \\
&amp; \text{Left-tailed: } &amp;&amp; \text{H}_1: \mu_1 - \mu_2&lt;0, \; \text{or }\; \\
&amp; \text{Right-tailed: } &amp;&amp; \text{H}_1: \mu_1 - \mu_2&gt;0\, .
\end{aligned}
\]</span></p>
<p>Test statistic:
<span class="math display">\[
t = \frac{\overline{x_1}-\overline{x_2}}{se(\overline{x_1}-\overline{x_2})} \sim t(n_1+n_2-2) ,
\]</span>
which follows a <em>t</em>-distribution with <span class="math inline">\((n_1+n_2-2)\)</span> degrees of freedom, where
<span class="math display">\[
se(\overline{x_1}-\overline{x_2}) = s_p\sqrt{\left(\frac{1}{n_1}+\frac{1}{n_2}\right)}
\]</span>
with
<span class="math display">\[
s_p = \sqrt{\frac{n_1-1}{n_1+n_2-2}\cdot s_1^2 + \frac{n_2-1}{n_1+n_2-2}\cdot s_2^2}.
\]</span>
where <span class="math inline">\(s_p^2\)</span> is called the <em>pooled sample variance</em>. It is a weighted average of the two sample variances <span class="math inline">\(s_1^2\)</span> and <span class="math inline">\(s_2^2\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
s_1^2 &amp;= \frac{\sum_{i=1}^{n_1}(x_i-\overline{x}_1)^2}{n_1-1} \\
s_2^2 &amp;= \frac{\sum_{i=1}^{n_2}(x_i-\overline{x}_1)^2}{n_2-1} \\
\end{aligned}
\]</span>
where <span class="math inline">\(\overline{x}_1\)</span> and <span class="math inline">\(\overline{x}_2\)</span> are the sample means.</p>
<p>The following are rejection regions corresponding to various alternative hypotheses.</p>
<table style="border-collapse: collapse; margin-left: auto; margin-right: auto; text-align: center">
<thead>
<tr style="border-bottom: 1pt solid #D3D3D3;">
<th style="width: 250px; ">
Alternative Hypothesis
</th>
<th style="width: 250px; ">
Rejection Region
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<span class="math inline">\(\text{H}_1: \mu_1 - \mu_2 \ne 0\)</span>
</td>
<td>
<span class="math inline">\(\vert t\vert &gt; t_{\alpha/2,\, v}\)</span>
</td>
</tr>
<tr>
<td>
<span class="math inline">\(\text{H}_1: \mu_1 - \mu_2&lt;0\)</span>
</td>
<td>
<span class="math inline">\(t&lt;-t_{ \alpha,\, v}\)</span>
</td>
</tr>
<tr>
<td>
<span class="math inline">\(\text{H}_1: \mu_1 - \mu_2&gt; 0\)</span>
</td>
<td>
<span class="math inline">\(t &gt; t_{\alpha,\, v}\)</span>
</td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(v=n_1+n_2-2\)</span> is the degrees of freedom.</p>
<p>The <em>t-</em>test under equal population variances is also known as the “pooled two-sample <em>t</em>-test”.</p>
<p>The <span class="math inline">\((1-\alpha)\)</span> (two-sided) confidence interval is given by:</p>
<p><span class="math display">\[
(\overline{x_1}-\overline{x_2})\pm t_{\alpha/2, v} \cdot se(\overline{x_1}-\overline{x_2}) \,.
\]</span></p>
<hr />
</div>
<div id="when-population-variances-are-not-equal" class="section level2">
<h2>When Population Variances Are Not Equal</h2>
<p>Let’s assume we have two <em>independent</em> samples from two normal distributions with <strong>unequal variances</strong>, <span class="math inline">\(\sigma_1^2\ne\sigma_2^2\)</span>.</p>
<p>Then we have the following test statistic:</p>
<p><span class="math display">\[
t = \frac{\overline{x_1}-\overline{x_2}}{se(\overline{x_1}-\overline{x_2})} \sim t(r) ,
\]</span>
which follows a <em>t</em>-distribution with the adjusted degrees of freedom <span class="math inline">\(r\)</span>, where
<span class="math display">\[
se(\overline{x_1}-\overline{x_2}) = \sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}} \, ,
\]</span>
and
<span class="math display">\[
r=\frac{\left(\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}\right)^2}{\frac{(s_1^2/n_1)^2}{n_1-1} + \frac{(s_2^2/n_2)^2}{n_2-1}}
\]</span>
If <span class="math inline">\(r\)</span> does not equal an integer, as it usually does not, then we take the integer portion of <span class="math inline">\(r\)</span>. That is, we use <span class="math inline">\(\lfloor r \rfloor\)</span> if necessary.</p>
<p>The <em>t-</em>test under unequal population variances is also known as the “Welch’s <em>t</em>-test”.</p>
<p>Note that when <em>each sample size is large</em>, for instance, <span class="math inline">\(n_1\geq 30\)</span> and <span class="math inline">\(n_2\geq 30\)</span>, the test statistic approximate the <em>standard normal distribution</em>. Then we could use the critical value under standard normal, <span class="math inline">\(z_{\alpha/2}\)</span>.</p>
<hr />
</div>
</div>
<div id="interactions" class="section level1">
<h1>Interactions</h1>
<p>Come back to our Current Population Survey (CPS) dataset and earnings regressions.</p>
<p>Now suppose we have two dummy variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
x_{1} &amp;= \begin{cases}
1 &amp; \text{ if sex=woman} \\
0 &amp; \text{ otherwise}
\end{cases} \\
x_2 &amp;= \begin{cases}
1 &amp; \text{ if the person is married} \\
0 &amp; \text{ otherwise}
\end{cases}
\end{aligned}
\]</span></p>
<p>We consider the following regression model:
<span class="math display">\[
y=\beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2 + \varepsilon .  
\]</span>
Here we have <span class="math inline">\(x_3=x_1x_2\)</span>, i.e., the product of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>.</p>
<ul>
<li>We often describe <span class="math inline">\(\beta_3\)</span> as measuring the <span style="color:#008B45FF">interaction</span> between the two dummy variables, or the <span style="color:#008B45FF">interaction effect</span>, and describe</li>
<li><span class="math inline">\(\beta_3 = 0\)</span> as the case when the interaction effect is zero.</li>
</ul>
<p>The conditional mean given <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> (<span class="math inline">\(E[y|x_1,x_2]\)</span>) takes at most four possible values:</p>
<table class="table table-striped table-hover" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
Scenario
</th>
<th style="text-align:right;">
<span class="math inline">\(x_1\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(x_2\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(E[y|x_1,x_2]\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
<span class="math inline">\(\mu_{00}\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
<span class="math inline">\(\mu_{10}\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
<span class="math inline">\(\mu_{01}\)</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
<span class="math inline">\(\mu_{11}\)</span>
</td>
</tr>
</tbody>
</table>
<p>We want to build a relationship between the four <span class="math inline">\(\beta\)</span>’s and the four <span class="math inline">\(\mu\)</span>’s. Plugging in the four scenarios, we have the following system of equations:
<span class="math display">\[
\begin{cases}
\beta_0 &amp;= \mu_{00} \\
\beta_0 + \beta_1 &amp;= \mu_{10} \\
\beta_0 + \beta_2 &amp;= \mu_{01} \\
\beta_0 + \beta_1 + \beta_2 + \beta_3 &amp;= \mu_{11} \\
\end{cases} .
\]</span></p>
<p>Solving the system of equations gives us
<span class="math display">\[
\begin{cases}
\color{#008B45FF}{\beta_0} &amp;= \mu_{00} \\
\color{red}{\beta_1} &amp;= \mu_{10} - \mu_{00} \\
\color{#1F77B4FF}{\beta_2} &amp;= \mu_{01} - \mu_{00} \\
\color{#9467BDFF}{\beta_3} &amp;= \mu_{11} - \mu_{10} - \mu_{01} + \mu_{00} \\
\end{cases} .
\]</span></p>
<p>These relationships can be summarized in the following table.</p>
<div class="text-center" style="width:100%">
<table style="border-collapse: collapse; margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td style="width: 50px;">
</td>
<td style="width: 150px;">
</td>
<td colspan="2" style="text-align:center; border-bottom:1pt solid #D3D3D3; background-color: rgba(239, 239, 66, 0.5);">
<span class="math inline">\(x_2\)</span>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td>
</td>
<td style="width: 250px; text-align:center; border:1pt solid #D3D3D3; font-weight: bold; background-color: rgba(239, 239, 66, 0.2);">
Unmarried (<span class="math inline">\(x_2=0\)</span>)
</td>
<td style="width: 250px; text-align:center; border:1pt solid #D3D3D3; font-weight: bold; background-color: rgba(239, 239, 66, 0.8);">
Married (<span class="math inline">\(x_2=1\)</span>)
</td>
<td style="width: 200px; text-align:center; font-weight: bold;">
Difference
</td>
</tr>
<tr>
<td rowspan="2" style="border-right:1pt solid #D3D3D3; background-color: rgba(150, 212, 212, 0.6);">
<span class="math inline">\(x_1\)</span>
</td>
<td style="border:1pt solid #D3D3D3; font-weight: bold; background-color: rgba(150, 212, 212, 0.2);">
Male (<span class="math inline">\(x_1=0\)</span>)
</td>
<td style="text-align:center; border:1pt solid #D3D3D3;">
<span class="math inline">\(\mu_{00}=\color{#008B45FF}{\beta_0}\)</span>
</td>
<td style="text-align:center; border:1pt solid #D3D3D3;">
<span class="math inline">\(\mu_{01}\)</span>
</td>
<td>
<span class="math inline">\(\mu_{01}-\mu_{00} = \color{#1F77B4FF}{\beta_2}\)</span>
</td>
</tr>
<tr>
<td style="border:1pt solid #D3D3D3; font-weight: bold; background-color: rgba(150, 212, 212, 0.8);">
Female (<span class="math inline">\(x_1=1\)</span>)
</td>
<td style="text-align:center; border:1pt solid #D3D3D3;">
<span class="math inline">\(\mu_{10}\)</span>
</td>
<td style="text-align:center; border:1pt solid #D3D3D3;">
<span class="math inline">\(\mu_{11}\)</span>
</td>
<td>
<span class="math inline">\(\mu_{11}-\mu_{10}=\color{#1F77B4FF}{\beta_2}+\color{#9467BDFF}{\beta_3}\)</span>
</td>
</tr>
<tr>
<td>
</td>
<td style="font-weight: bold;">
Difference
</td>
<td style="text-align:center;">
<span class="math inline">\(\mu_{10}-\mu_{00} = \color{red}{\beta_1}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\mu_{11}-\mu_{01} = \color{red}{\beta_1} + \color{#9467BDFF}{\beta_3}\)</span>
</td>
<td>
</td>
</tr>
</tbody>
</table>
</div>
<p>We can view</p>
<ul>
<li>the regression intercept <span class="math inline">\(\color{#008B45FF}{\beta_0}\)</span> as the expected earnings for unmarried men;</li>
<li>the coefficient <span class="math inline">\(\color{red}{\beta_1}\)</span> as the effect of sex on expected earnings for unmarried wage earners;</li>
<li>the coefficient <span class="math inline">\(\color{#1F77B4FF}{\beta_2}\)</span> as the effect of marriage on expected earnings for men wage earners;</li>
<li>the coefficient <span class="math inline">\(\color{#9467BDFF}{\beta_3}\)</span> has two equivalent interpretations:
<ul>
<li><span class="math inline">\((\mu_{11} - \mu_{10}) - (\mu_{01} - \mu_{00})\)</span> stands for difference between the effects of marriage on expected earnings among women and men, or rearranging the terms,</li>
<li><span class="math inline">\((\mu_{11} - \mu_{01}) - (\mu_{10} - \mu_{00})\)</span> stands for difference between the effects of sex on expected earnings among married and non-married wage earners.</li>
</ul></li>
</ul>
<p>Back to our CPS earning regression example.</p>
<pre class="r"><code># prepare the dataset
cps_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/my1396/course_dataset/refs/heads/main/cps09mar.csv&quot;)
cps_data &lt;- cps_data %&gt;% 
    mutate(wage =  earnings/(hours*week),
           lwage = log(wage), 
           female = factor(female),
           married = ifelse(marital&lt;4, 1, 0) %&gt;% factor()
           )
cps_data %&gt;% select(lwage, female, married)</code></pre>
<pre><code>## # A tibble: 50,742 × 3
##    lwage female married
##    &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;  
##  1  4.13 0      1      
##  2  3.06 0      1      
##  3  2.75 0      1      
##  4  3.12 1      1      
##  5  4.13 0      1      
##  6  2.76 1      0      
##  7  2.78 0      1      
##  8  2.78 1      1      
##  9  3.65 0      1      
## 10  2.73 1      1      
## # ℹ 50,732 more rows</code></pre>
<pre class="r"><code>dummy_lm &lt;- lm(lwage~female+married+female:married, data=cps_data)
summary(dummy_lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = lwage ~ female + married + female:married, data = cps_data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11.0081  -0.3779   0.0014   0.3855   2.7890 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       2.794187   0.007211 387.513  &lt; 2e-16 ***
## female1          -0.062195   0.010023  -6.205 5.51e-10 ***
## married1          0.350636   0.008510  41.205  &lt; 2e-16 ***
## female1:married1 -0.216118   0.012422 -17.398  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6537 on 50738 degrees of freedom
## Multiple R-squared:  0.06475,    Adjusted R-squared:  0.0647 
## F-statistic:  1171 on 3 and 50738 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Summarize the output in a regression table.</p>
<table style="text-align:center">
<caption>
<strong>Regression including dummay variables and interactions</strong>
</caption>
<tr>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
<em>Dependent variable:</em>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="1" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
lwage
</td>
</tr>
<tr>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
female1
</td>
<td>
-0.062<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.010)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
married1
</td>
<td>
0.351<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.009)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
female1:married1
</td>
<td>
-0.216<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.012)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Constant
</td>
<td>
2.794<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.007)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
Observations
</td>
<td>
50,742
</td>
</tr>
<tr>
<td style="text-align:left">
R<sup>2</sup>
</td>
<td>
0.065
</td>
</tr>
<tr>
<td style="text-align:left">
Adjusted R<sup>2</sup>
</td>
<td>
0.065
</td>
</tr>
<tr>
<td style="text-align:left">
Residual Std. Error
</td>
<td>
0.654 (df = 50738)
</td>
</tr>
<tr>
<td style="text-align:left">
F Statistic
</td>
<td>
1,170.951<sup>***</sup> (df = 3; 50738)
</td>
</tr>
<tr>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
<em>Note:</em>
</td>
<td style="text-align:right">
<span>***</span>: p&lt;0.01; <span>**</span>: <strong>p&lt;0.05</strong>; <span>*</span>: p&lt;0.1
</td>
</tr>
</table>
<p>Plugging in the estimates, we write the regression model as</p>
<p><span class="math display">\[
\widehat{\log (wage)} = 2.79 - 0.06\, Female + 0.35\, Married - 0.22\, Female \times Married
\]</span>
Note that we have the <strong>log transformation</strong> for the wage variable. This changes the interpretation of the coefficients. The coefficient can be interpreted as a one-unit change in the independent variable is associated with a 100 times the coefficient percent change in the (original) dependent variable.</p>
<p>The regression results show that</p>
<ul>
<li><p>the intercept <span class="math inline">\(\color{#008B45FF}{\beta_0}\)</span> (<span class="math inline">\(2.79\)</span>) indicates that the expected log(wage) for unmarried men is <span class="math inline">\(2.79\)</span>;</p></li>
<li><p>the coefficient <span class="math inline">\(\color{red}{\beta_1}\)</span> (<span class="math inline">\(-0.06\)</span>) indicates that the effects of sex on expected earnings for unmarried wage earners is <span class="math inline">\(6\%\)</span> lower.</p>
<ul>
<li>That is, unmarried female earn <span class="math inline">\(6\%\)</span> less than unmarried male.</li>
</ul></li>
<li><p>the coefficient <span class="math inline">\(\color{#1F77B4FF}{\beta_2}\)</span> (<span class="math inline">\(0.35\)</span>) indicates that the effects of marriage on expected earnings for males is <span class="math inline">\(35\%\)</span>.</p>
<ul>
<li>That is married male earns <span class="math inline">\(35\%\)</span> more than unmarried male.</li>
</ul></li>
<li><p>the coefficient <span class="math inline">\(\color{#9467BDFF}{\beta_3}\)</span> (<span class="math inline">\(-0.22\)</span>) indicates that</p>
<ul>
<li><span class="math inline">\(\beta_3=(\mu_{11} - \mu_{10}) - (\mu_{01} - \mu_{00})\)</span>: difference between the effects of marriage on expected earnings among women and men.
<ul>
<li>effects of marriage for women is <span class="math inline">\(-22\%\)</span> less than men;</li>
<li>the effects of marriage on expected earnings for females is <span class="math inline">\(11\%=(35-22)\%\)</span>;</li>
<li>married females earns <span class="math inline">\(11\%\)</span> more than unmarried females.</li>
</ul></li>
<li><span class="math inline">\(\beta_3=(\mu_{11} - \mu_{01}) - (\mu_{10} - \mu_{00})\)</span>: difference between the effects of sex on expected earnings among married and non-married wage earners.
<ul>
<li>effects of sex for married is <span class="math inline">\(22\%\)</span> less than unmarried.</li>
<li>the effects for sex on expected earnings for married is <span class="math inline">\(-28\%=(-6-22)\%\)</span>;</li>
<li>married females earn <span class="math inline">\(28\%\)</span> less than married males.</li>
</ul></li>
</ul></li>
</ul>
<p>Visualization of <span class="math inline">\(\color{red}{\beta_1}\)</span>, the effects of sex on expected earnings for unmarried wage earners.</p>
<pre class="r"><code>library(ggridges)
ggplot(cps_data %&gt;% filter(married==0), aes(x = lwage, y = female, group = female)) +
    geom_density_ridges(jittered_points = TRUE,
                        position = position_points_jitter(height = 0),
                        quantile_lines=TRUE,
                        quantile_fun = function(x,...) median(x),
                        point_size = 1.5,
                        point_shape = 1,
                        alpha = 0.3) </code></pre>
<p><img src="08_Oct_02_lecture_files/figure-html/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Visualization of <span class="math inline">\(\color{#1F77B4FF}{\beta_2}\)</span>, the effect of marriage on expected earnings for male wage earners.</p>
<pre class="r"><code>ggplot(cps_data %&gt;% filter(female==0), aes(x = lwage, y = married, group = married)) +
    geom_density_ridges(jittered_points = TRUE,
                        position = position_points_jitter(height = 0),
                        quantile_lines=TRUE,
                        quantile_fun = function(x,...) median(x),
                        point_size = 1.5,
                        point_shape = 1,
                        alpha = 0.3) </code></pre>
<p><img src="08_Oct_02_lecture_files/figure-html/unnamed-chunk-12-1.png" width="672" style="display: block; margin: auto;" /></p>
<hr />
</div>
<div id="two-way-anova-test" class="section level1">
<h1>Two-way ANOVA test</h1>
<p>The two-way ANOVA test is used to compare the fits of two regression models.</p>
<ul>
<li><p>Test for Improvement: It tests whether adding more predictors (variables) to a model significantly improves the model’s ability to explain the variability in the response variable.</p></li>
<li><p>Model Selection: Helps in deciding between a simpler model with fewer variables and a more complex one with more variables.</p></li>
</ul>
<p><strong>When to Use ANOVA for Comparing Regression Models?</strong></p>
<ul>
<li><p>Nested Models: Applicable when you have two nested models - one is a special case of the other. For example, Model 1 might include predictors <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, while Model 2 includes <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span>.</p></li>
<li><p>Same Response Variable: Both models must be trying to predict the same response variable.</p></li>
<li><p>Linear Models: Typically used for comparing linear regression models.</p></li>
</ul>
<p><strong>How to perform the comparison?</strong></p>
<ol style="list-style-type: decimal">
<li><p>Fit both models: Fit the simpler model and the more complex model to your data.</p></li>
<li><p>Conduct ANOVA test: Use an ANOVA test to compare the models. In R, this can be done using the <code>anova()</code> function.</p></li>
<li><p>Interpret the results: If the <span class="math inline">\(p\)</span>-value from the ANOVA test is low (typically &lt;0.05), it suggests that the more complex model provides a significantly better fit to the data.</p></li>
</ol>
<p><strong>Mathematical formulations</strong></p>
<p>For instance, consider we have two models:</p>
<ul>
<li>Model 1: <span class="math inline">\(y=\beta_1 + \beta_2 x_{2} + \varepsilon\)</span>, and</li>
<li>Model 2: <span class="math inline">\(y=\beta_1 + \beta_2 x_{2} + \beta_3 x_3 + \beta_3 x_4 + \varepsilon\)</span>.</li>
</ul>
<p>We can form a test by comparing the <span class="math inline">\(R^2\)</span> from the two models.</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\text{H}_0: \beta_3=\beta_4=0 \\
&amp;\text{H}_1: \beta_3\ne 0 \text{ or } \beta_4\ne 0 ,
\end{aligned}
\]</span>
which imposes two exclusion restrictions.</p>
<p>Note that this is similar to the <span class="math inline">\(F\)</span>-test for the significance of the overall regression, which test for
<span class="math display">\[
\begin{aligned}
&amp;\text{H}_0: \beta_2=\beta_3=\cdots=\beta_K=0 \\
&amp;\text{H}_1: \text{At least one of the } \beta_2, \beta_3,\ldots, \beta_K,\text{is not zero.}
\end{aligned}
\]</span></p>
<p>In stead of having a <span class="math inline">\(F(K-1, n-K)\)</span> test statistic, we can form a <span class="math inline">\(F(p, n-K)\)</span> test statistic, where <span class="math inline">\(p\)</span> is the number of exclusion restrictions. In this case, we have <span class="math inline">\(p=2\)</span>.</p>
<p>To fix ideas, we call Model 1 as the “restricted model”, and Model 2 the “unrestricted model” or the “full model”.</p>
<p>Denote</p>
<ul>
<li>the <span class="math inline">\(R^2\)</span> obtained from the restricted model by <span class="math inline">\(R_R^2\)</span>, and</li>
<li>the <span class="math inline">\(R^2\)</span> obtained from the unrestricted model by <span class="math inline">\(R_U^2\, .\)</span></li>
<li>Noting that <span class="math inline">\(R_U^2 &gt; R_R^2\)</span>.</li>
</ul>
<p>The test statistic is given by</p>
<p><span class="math display">\[
F = \left(\frac{n-K}{p}\right) \left(\frac{R_U^2-R_R^2}{1-R_U^2}\right) \sim F(p, n-K) .
\]</span></p>
<p>The null hypothesis is rejected if the exclusion of these <span class="math inline">\(p\)</span> explanatory variables results in a sufficiently large fall in the <span class="math inline">\(R^2\)</span> goodness of fit measure.</p>
<pre class="r"><code>dummy_lm_restricted &lt;- lm(lwage~female+married, data=cps_data)
summary(dummy_lm_restricted)</code></pre>
<pre><code>## 
## Call:
## lm(formula = lwage ~ female + married, data = cps_data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.9795  -0.3814   0.0047   0.3947   2.7494 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.867008   0.005889  486.85   &lt;2e-16 ***
## female1     -0.202913   0.005938  -34.17   &lt;2e-16 ***
## married1     0.249212   0.006218   40.08   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6556 on 50739 degrees of freedom
## Multiple R-squared:  0.05917,    Adjusted R-squared:  0.05914 
## F-statistic:  1596 on 2 and 50739 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Summarize the unrestricted and restricted models in a table.</p>
<table style="text-align:center">
<caption>
<strong>Regression Results for bone density</strong>
</caption>
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td colspan="2">
<em>Dependent variable:</em>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td colspan="2">
lwage
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(1)
</td>
<td>
(2)
</td>
</tr>
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
female1
</td>
<td>
-0.203<sup>***</sup>
</td>
<td>
-0.062<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.006)
</td>
<td>
(0.010)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
married1
</td>
<td>
0.249<sup>***</sup>
</td>
<td>
0.351<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.006)
</td>
<td>
(0.009)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
female1:married1
</td>
<td>
</td>
<td>
-0.216<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
(0.012)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Constant
</td>
<td>
2.867<sup>***</sup>
</td>
<td>
2.794<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.006)
</td>
<td>
(0.007)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
Observations
</td>
<td>
50,742
</td>
<td>
50,742
</td>
</tr>
<tr>
<td style="text-align:left">
R<sup>2</sup>
</td>
<td>
0.059
</td>
<td>
0.065
</td>
</tr>
<tr>
<td style="text-align:left">
Adjusted R<sup>2</sup>
</td>
<td>
0.059
</td>
<td>
0.065
</td>
</tr>
<tr>
<td style="text-align:left">
Residual Std. Error
</td>
<td>
0.656 (df = 50739)
</td>
<td>
0.654 (df = 50738)
</td>
</tr>
<tr>
<td style="text-align:left">
F Statistic
</td>
<td>
1,595.591<sup>***</sup> (df = 2; 50739)
</td>
<td>
1,170.951<sup>***</sup> (df = 3; 50738)
</td>
</tr>
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
<em>Note:</em>
</td>
<td colspan="2" style="text-align:right">
<span>***</span>: p&lt;0.01; <span>**</span>: <strong>p&lt;0.05</strong>; <span>*</span>: p&lt;0.1
</td>
</tr>
</table>
<p> </p>
<p><strong>Two-way Anova test</strong></p>
<pre class="r"><code># Compare the restricted with the full model
anova(dummy_lm_restricted, dummy_lm)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: lwage ~ female + married
## Model 2: lwage ~ female + married + female:married
##   Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    
## 1  50739 21808                                 
## 2  50738 21679  1    129.33 302.7 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<ul>
<li>The result shows a <span class="math inline">\(df\)</span> of 1 (indicating that the unrestricted model has one additional parameter, i.e., <span class="math inline">\(p=1\)</span>), and a very small <span class="math inline">\(p\)</span>-value (<span class="math inline">\(&lt; .001\)</span>).</li>
<li>This means that adding the interaction to the model did lead to a significantly improved fit over the model 1.</li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML.js";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
