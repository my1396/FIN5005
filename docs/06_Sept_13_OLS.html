<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Menghan Yuan" />


<title>OLS – Lecture</title>

<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Course Website</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">OLS – Lecture</h1>
<h4 class="author">Menghan Yuan</h4>
<h4 class="date">Sept 11, 2024</h4>

</div>


<SCRIPT language="JavaScript" SRC="my_jxscript.js"></SCRIPT>
<p><a class="top-link" href="#top" id="js-top">↑</a></p>
<div id="linear-regression-model" class="section level1">
<h1>Linear Regression Model</h1>
<p>The multiple linear regression model is used to study the relationship between a <span style="color:#337ab7">dependent variable</span> and one or more <span style="color:#337ab7">independent variables</span>. The <strong>generic form of the linear regression model</strong> is</p>
<p><span class="math display" id="eq:scalar-form">\[
\begin{equation}
\begin{aligned}
y_i &amp;= f(x_{1i}, x_{2i}, \ldots, x_{Ki}) + \varepsilon_i \\
&amp;= x_{1i} \beta_1 + x_{2i} \beta_2 + \cdots + x_{Ki} \beta_K + \varepsilon_i
\end{aligned}\tag{1}
\end{equation}
\]</span>
where</p>
<ul>
<li><span class="math inline">\(y_i\)</span> is the <span style="color:#337ab7">dependent</span> or <span style="color:#337ab7">explained</span> variable for observation <span class="math inline">\(i\)</span>;</li>
<li><span class="math inline">\(x_{1i}, x_{2i}, \ldots, x_{Ki}\)</span> are the <span style="color:#337ab7">independent</span> or <span style="color:#337ab7">explanatory variables</span>, also called the <span style="color:#337ab7">regressors</span>, <span style="color:#337ab7">predictors</span>, or <span style="color:#337ab7">covariates</span>.</li>
<li><span class="math inline">\(\beta_1, \beta_2, \ldots, \beta_K\)</span> are the unknown parameters that we want to estimate.</li>
</ul>
<p>and a sample of <span class="math inline">\(n\)</span> observations indexed <span class="math inline">\(i = 1, 2, \ldots, n.\)</span></p>
<p>Eq. <a href="#eq:scalar-form">(1)</a> is referred to as the scalar form.</p>
<p>We usually let <span class="math inline">\(x_{1i}=1\)</span> for all <span class="math inline">\(i=1, 2, \ldots, n,\)</span>; where the parameter <span class="math inline">\(\beta_1\)</span> corresponds to the <span style="color:#008B45FF">intercept</span>.</p>
<p>The error term <span class="math inline">\(\varepsilon_i\)</span> is a random disturbance, so named because it “disturbs” an otherwise stable relationship, hence also called as “disturbances.”<br />
<span class="math inline">\(\varepsilon_i\)</span> reflects the combined effect of all additional influences on the outcome variable <span class="math inline">\(y_i\)</span>.</p>
<p>Several reasons why we need the error term <span class="math inline">\(\varepsilon_i\)</span>:</p>
<ul>
<li>the effect of relevant variables not included in our set of <span class="math inline">\(K\)</span> explanatory variables (omitted variables);</li>
<li>any non-linear effects of our included variables <span class="math inline">\((x_{1i}, x_{2i}, \ldots, x_{Ki})\)</span>;</li>
<li>measurement errors: For example, the difficulty of obtaining reasonable measures of incomes and expenditures. People might overstate incomes to make it look better.</li>
<li>the factor is not observable. For instance, the aptitude or soft skills of people affect their income levels.</li>
</ul>
<p><strong>Notation</strong></p>
<p><span style="color:#008B45FF">The scalar form</span>
<span class="math display">\[
\color{#008B45FF}{y_i = x_{1i} \beta_1 + x_{2i} \beta_2 + \cdots + x_{Ki} \beta_K + \varepsilon_i}
\]</span>
can be written in <span style="color:#008B45FF">vector form</span>:</p>
<p><span class="math display">\[
\color{#008B45FF}{y_i = \boldsymbol{x}_i&#39;\boldsymbol{\beta} + \varepsilon_i}
\]</span>
where <span class="math inline">\(\boldsymbol{x}_i\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span> denote the <span class="math inline">\(K\times 1\)</span> column vectors:</p>
<p><span class="math display">\[
\boldsymbol{x}_i = \begin{pmatrix}
x_{1i} \\
x_{2i} \\
\vdots \\
x_{Ki}
\end{pmatrix}_{K\times 1} \quad \text{and} \quad
\boldsymbol{\beta} = \begin{pmatrix}
\beta_{1} \\
\beta_{2} \\
\vdots \\
\beta_{K}
\end{pmatrix}_{K\times 1}
\]</span>
Sometimes you may see the <span class="math inline">\(i\)</span> subscripts omitted and the model written as
<span class="math display">\[
y = \boldsymbol{x}&#39;\boldsymbol{\beta} + \varepsilon
\]</span>
for a typical observation.</p>
<p>We can also stack the observations for a sample of size <span class="math inline">\(n\)</span>, and express the linear regression model in <span style="color:#008B45FF">matrix form</span>:
<span class="math display">\[
\color{#008B45FF}{\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}} ,
\]</span>
where</p>
<p><span class="math display">\[
\boldsymbol{y} = \begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix}_{n\times 1},   \quad
\boldsymbol{X} = \begin{pmatrix}
x_1&#39;\\
x_2&#39;\\
\vdots \\
x_n&#39;\\
\end{pmatrix}
= \begin{pmatrix}
x_{11} &amp; x_{21} &amp;  \cdots &amp; x_{K1} \\
x_{12} &amp; x_{22} &amp; \cdots &amp; x_{K2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{1n} &amp; x_{2n} &amp; \cdots &amp; x_{Kn}
\end{pmatrix}_{n\times K},    \quad
\boldsymbol{\beta} = \begin{pmatrix}
\beta_{1} \\
\beta_{2} \\
\vdots \\
\beta_{K}
\end{pmatrix}_{K\times 1},   \quad \text{and }\;
\varepsilon = \begin{pmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_n
\end{pmatrix}_{n\times 1}.
\]</span>
<span class="math inline">\(\boldsymbol{y}\)</span> and <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> are <span class="math inline">\(n\times 1\)</span> column vectors and <span class="math inline">\(X\)</span> is an <span class="math inline">\(n\times K\)</span> matrix.</p>
<p>Linear models often have an intercept term. We may have <span class="math inline">\(x_{1i}=1,\)</span> for all <span class="math inline">\(i=1,\ldots, n,\)</span>, in which case the parameter <span class="math inline">\(\beta_1\)</span> correspond to the intercept.</p>
<hr />
<p><strong>A notational convention</strong></p>
<p>We use lower-case <span class="math inline">\(\boldsymbol{x}\)</span> to denote either a column or a row of <span class="math inline">\(\boldsymbol{X}\)</span>, you should tell which it refers to from the context.</p>
<ul>
<li>We use subscripts <span class="math inline">\(i\)</span> and <span class="math inline">\(t\)</span> to denote row observations of <span class="math inline">\(\boldsymbol{X}\)</span>, and</li>
<li>subscripts <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> to denote columns (variables) of <span class="math inline">\(\boldsymbol{X}\)</span>.</li>
</ul>
<p>For instance,
<span class="math display">\[
y_i = \boldsymbol{x}_i&#39;\boldsymbol{\beta} + \varepsilon_i
\]</span>
<span class="math inline">\(\boldsymbol{x}_i,\)</span> <span class="math inline">\(i=1,\ldots,n,\)</span> denotes the <span class="math inline">\(i^{\text{th}}\)</span> <span style="color:#337ab7">row</span> of <span class="math inline">\(\boldsymbol{X}\)</span>, referring to a single observation <span class="math inline">\(i\)</span>.</p>
<p><span class="math display">\[
\boldsymbol{x}_i = \begin{pmatrix}
x_{1i} \\
x_{2i} \\
\vdots \\
x_{Ki}
\end{pmatrix}_{K\times 1} \quad \text{and} \quad
X=\begin{pmatrix}
x_1&#39; \\
x_2&#39; \\
\vdots \\
x_n&#39; \\
\end{pmatrix}_{n\times K}
\]</span></p>
<p>Sometimes we see the following form:
<span class="math display">\[
\color{#008B45FF}{\boldsymbol{y} = \beta_1 \boldsymbol{x}_1 + \beta_2 \boldsymbol{x}_2 + \ldots + \beta_K \boldsymbol{x}_K + \varepsilon}
\]</span>
Here <span class="math inline">\(\boldsymbol{x}_k,\)</span> <span class="math inline">\(k=1,\ldots, K,\)</span> denotes the <span class="math inline">\(k^{\text{th}}\)</span> <span style="color:#337ab7">column</span> of <span class="math inline">\(\boldsymbol{X}\)</span>, referring to a single variable <span class="math inline">\(k\)</span>.</p>
<p><span class="math display">\[
\boldsymbol{x}_k = \begin{pmatrix}
x_{k1} \\
x_{k2} \\
\vdots \\
x_{kn}
\end{pmatrix}_{n\times 1} \quad \text{and} \quad
X = \begin{pmatrix}x_1, x_2, \ldots, x_K\end{pmatrix}_{n\times K}
\]</span></p>
<p>Note that we typically use <strong>boldface</strong> to indicate vectors and matrices, and regular typeface for scalars.<br />
But this convention is only loosely followed as it can be cumbersome for typing.</p>
<hr />
</div>
<div id="ordinary-least-squares-ols" class="section level1">
<h1>Ordinary Least Squares (OLS)</h1>
<p>The OLS estimator finds the parameter vector <span class="math inline">\(\boldsymbol{\beta}\)</span> which minimizes the sum of the squared errors, <span class="math inline">\(\sum_{i=1}^n \varepsilon_i^2\)</span>.</p>
<p>Formally
<span class="math display">\[
\hat{\boldsymbol{\beta}} = \arg\underset{\boldsymbol{\beta}}{\min} \sum_{i=1}^n \varepsilon_i^2
\]</span></p>
<p>In matrix form:
<span class="math display">\[
\begin{aligned}
\hat{\boldsymbol{\beta}} &amp;= \arg\underset{\boldsymbol{\beta}}{\min} \boldsymbol{\varepsilon}&#39;\boldsymbol{\varepsilon} \\
&amp;= \arg\underset{\boldsymbol{\beta}}{\min} (\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})&#39;(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})
\end{aligned}
\]</span></p>
<p>To obtain the OLS estimator, we minimize <span class="math inline">\(\phi(\boldsymbol{\beta})=\boldsymbol{\varepsilon}&#39;\boldsymbol{\varepsilon}\)</span> with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>, where <span class="math inline">\(\boldsymbol{\varepsilon}=\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}\)</span>.</p>
<p>Apply the first-order conditions:
<span class="math display" id="eq:normal-eqn">\[
\begin{equation} \tag{2}
\frac{\partial\phi(\beta)}{\partial\beta} = -2X&#39;(y-X\beta) = 0
\end{equation}
\]</span>
and the second-order conditions
<span class="math display">\[
\frac{\partial^2\phi(\beta)}{\partial\beta\partial\beta&#39;} = 2X&#39;X &gt;0,
\]</span>
confirming that we obtain a unique minimum if <span class="math inline">\((X&#39;X)^{-1}\)</span> exists.</p>
<p><a href="#eq:normal-eqn">(2)</a> is referred to as the “normal equations.”</p>
<p>The OLS estimator of <span class="math inline">\(\beta\)</span> is given by:
<span class="math display">\[
\color{#EE0000FF}{\hat{\beta}_{\text{OLS}} = (X&#39;X)^{-1}X&#39;y} .
\]</span>
Note that</p>
<ul>
<li><span class="math inline">\((X&#39;X)^{-1}\)</span> is a symmetric square <span class="math inline">\(K \times K\)</span> matrix.</li>
<li>For <span class="math inline">\((X&#39;X)^{-1}\)</span> to exist, we need <span class="math inline">\(X\)</span> to be full rank. This is referred to as the full rank assumption.</li>
</ul>
<div class="boxed">
<p><em>Assumption:</em> <span class="math inline">\(X\)</span> is an <span class="math inline">\(n\times K\)</span> matrix with full rank <span class="math inline">\(K\)</span>.</p>
</div>
<p>In plain language, full rank means there are no exact linear relationships among the variables.<br />
This assumption is known as the <span style="color:#337ab7">identification condition</span>.</p>
<p><span class="math inline">\((X&#39;X)^{-1}\)</span> may not exist. If this is the case, then this matrix is called <span style="color:#337ab7">non-invertible</span> or <span style="color:#337ab7">singular</span> and is said to be of <span style="color:#337ab7">less than full rank</span>.</p>
<p>There are two possible reasons why this matrix might be non-invertible.</p>
<ul>
<li>One, based on a trivial theorem about rank, is that <span class="math inline">\(n &lt; k\)</span> i.e., we have more independent variables than observations. This is unlikely to be a problem for us in practice.</li>
<li>The other is that one or more of the independent variables are a linear combination of the other variables i.e. <em>perfect</em> multicollinearity.</li>
</ul>
<div class="example">
<p><span id="exm:unnamed-chunk-1" class="example"><strong>Example 1  (Short Rank) </strong></span>Suppose we have the following linear model:
<span class="math display">\[
    y=\beta_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \varepsilon,
    \]</span>
where
<span class="math display">\[
    x_4 = x_2 + x_3 ,
    \]</span>
meaning there is an exact linear dependency in the model.</p>
<p>Aim: We want to show that <span class="math inline">\(\boldsymbol{\beta}&#39; = (\beta_1, \beta_2, \beta_3, \beta_4)\)</span> cannot be uniquely identified.</p>
<p>Now we let <span class="math inline">\(\boldsymbol{\tilde{\beta}}&#39; = (\beta_1, \tilde{\beta_2}+a, \tilde{\beta_3}+a, \tilde{\beta_4}-a)\)</span> for any constant <span class="math inline">\(a\)</span>.</p>
<p>Then we find that <span class="math inline">\(\boldsymbol{x}&#39;\boldsymbol{\beta} = \boldsymbol{x}&#39;\boldsymbol{\tilde{\beta}}\)</span>.</p>
<p>In other words, there is no way to estimate the parameters of this model.</p>
</div>
</div>
<div id="properties-of-ols-estimators" class="section level1">
<h1>Properties of OLS Estimators</h1>
<p>The residuals for observations <span class="math inline">\(i\)</span> is <span class="math inline">\(e_i=y_i-\hat{y}_i=y_i-x_i&#39;\hat{\beta}_{\text{OLS}}\)</span>.</p>
<p>Note that we distinguish the error term <span class="math inline">\(\varepsilon\)</span> from residuals <span class="math inline">\(e\)</span>.</p>
<ul>
<li>the error term <span class="math inline">\(\varepsilon\)</span> is from the population that cannot be observed;</li>
<li>residuals <span class="math inline">\(e\)</span> are from samples that can be observed.</li>
</ul>
<p>We will introduce essential properties of OLS estimators. These properties do not depend on any assumptions – they will always be true so long as we compute them by minimizing the sum of squared errors.</p>
<p>We plug in <span class="math inline">\(\hat{\beta}\)</span> to the normal equations:
<span class="math display">\[
(X&#39;X)\hat{\beta} = X&#39;y
\]</span>
Plug in <span class="math inline">\(y=X\hat{\beta}+e\)</span>.
<span class="math display">\[
\begin{split}
(X&#39;X)\hat{\beta} &amp;= X&#39;(X\hat{\beta}+e) \\
(X&#39;X)\hat{\beta} &amp;= X&#39;X\hat{\beta} + X&#39;e \\
X&#39;e &amp;= \boldsymbol{0}
\end{split}
\]</span>
or we can express in matrix form:
<span class="math display">\[
\begin{pmatrix}
x_1&#39; \\
x_2&#39; \\
\vdots \\
x_K&#39;
\end{pmatrix}_{K\times n} \cdot e_{n\times 1} =
\begin{pmatrix}
x_1&#39;e \\
x_2&#39;e \\
\vdots \\
x_K&#39;e
\end{pmatrix}_{K\times 1} =
\begin{pmatrix}
0 \\
0 \\
\vdots \\
0
\end{pmatrix}_{K\times 1}
\]</span>
This implies that, by construction, the OLS residuals <span class="math inline">\(\boldsymbol{e}\)</span> in our sample are <span style="color:#337ab7">uncorrelated</span> with each of the <span class="math inline">\(K\)</span> explanatory variables included in the model.</p>
<div class="proof">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span>Uncorrelated means that <span class="math inline">\(\text{Cov}(x_k, e)=0\)</span>.</p>
<p><span class="math display">\[
\begin{split}
\text{Cov}(x_k, e) &amp;= E\left\{\left[x_k-E(x_k)\right] \left[e-E(e)\right]^T \right\} \\
&amp;= E[x_ke^T] - E[x_k]E[e]^T \\
&amp;= \boldsymbol{0} - E[x_k] \boldsymbol{0} \\
&amp;= 0
\end{split}
\]</span></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML.js";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
